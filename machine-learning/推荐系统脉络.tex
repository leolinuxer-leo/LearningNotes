\documentclass[12pt]{article}
%\usepackage[utf8]{inputenc}
%\documentclass[UTF8]{ctexart}
%\usepackage[UTF8, heading = false, scheme = plain]{ctex}
\usepackage{geometry}
%geometry{a4paper,scale=0.9}
\geometry{a4paper,left=1cm,right=1cm,top=1cm,bottom=2cm}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{url}
%\usepackage{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage[linesnumbered,ruled,lined]{algorithm2e}
\usepackage{cite}
%\addbibresource{ref.bib}
%\bibliography{ref.bib}
\usepackage{caption}
\usepackage{graphicx, subfig}
\usepackage{float}
%\usepackage[fontset=ubuntu]{ctex}
%\usepackage{fontspec}
\usepackage{xeCJK}
%\usepackage[colorlinks,
%anchorcolor=black,
%citecolor=black]{hyperref}
%\setmainfont{SimSun}
\usepackage[section]{placeins}
\usepackage{enumitem}
\usepackage{framed}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{indentfirst}
\usepackage{setspace}%使用间距宏包
\linespread{1.5}

\title{推荐系统脉络\cite{Deep_Learning_Recommender_System}\cite{Recommender_System_With_Deep_Learning_Collection}}
\author{leolinuxer}
%\date{June 2020}

\begin{document}
%\setlength{\parindent}{0pt}
\maketitle
\tableofcontents

\part{推荐系统的基本概念}
\section{推荐系统的系统架构}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Deep_Learning_Refenrence_System_Architecture.jpg}
\end{figure}

\part{前深度学习时代CTR预估模型的演化之路}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Traditional_CTR_Model_Evolution.jpg}
\end{figure}

看到上面的关系图，有经验的同学可能已经对各模型的细节和特点如数家珍了。中间位置的LR模型向四个方向的延伸分别代表了传统CTR模型演化的四个方向。
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item 向下为了解决\textbf{特征交叉}的问题，演化出PLOY2，FM，FFM等模型；
    \item 向右为了使用模型化、自动化的手段解决之前\textbf{特征工程}的难题，Facebook将LR与GBDT进行结合，提出了GBDT+LR组合模型；
    \item 向左Google从\textbf{online learning}的角度解决模型时效性的问题，提出了FTRL；
    \item 向上阿里基于\textbf{样本分组}的思路增加模型的非线性，提出了LS-PLM（MLR）模型。
\end{itemize}

\section{协同过滤——经典的推荐算法}
论文：Amazon.com Recommenders Item-to-Item Collaborative Filtering

\subsection{基于用户的协同过滤}
基于用户的协同过滤算法的基本思想是：当召回用户A的候选集时，可以先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而用户A没有未交互的物品作为候选集。

因此，我们首先需要计算两个用户的兴趣相似度。给定用户$u$和用户$v$，令$N(u)$表示用户$u$曾经有过正反馈的物品集合，令$N(v)$ 为用户$v$曾经有过正反馈的物品集合。那么我们可以通过以下两种方法计算用户的相似度：
$$
w_{uv} = \frac{|N(u) \cap N(v)|}{N(u) \cup N(v)} \qquad \qquad \text{基于 Jaccard 公式}
$$
$$
w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)| |N(v)|}} \qquad \qquad \text{基于余弦相似度}
$$

余弦相似度为什么是上面这种写法呢，因为这里，我们并不是用的用户对物品的评分，而是用的0-1表示，所以对两个集合做交集，相当于进行了点乘。如果我们的矩阵是用户对物品的评分，那么计算余弦相似度的时候可以利用用户的具体评分而不是0-1值。

如果简单的基于余弦相似度，显得过于粗糙，以图书为例，如果两个用户都曾经买过《新华字典》，这丝毫不能说明他们兴趣相似， 因为绝大多数中国人小时候都买过《新华字典》。但如果两个用户都买过《数据挖掘导论》，那可 以认为他们的兴趣比较相似，因为只有研究数据挖掘的人才会买这本书。换句话说，\textbf{两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度}，因此，我们可以基于物品的流行度对热门物品进行一定的惩罚：
$$
w_{uv} = \frac{\sum_{i \in N(u)\cap N(v)\frac{1}{\log 1 + |N(i)|}}}{\sqrt{|N(u)| |N(v)|}}
$$

得到用户之间的兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的$K$个用户喜欢的 物品。如下的公式度量了UserCF算法中用户$u$对物品$i$的感兴趣程度:
$$
p(u,i) = \sum_{v \in S(u,K) \cap N(i)} w_{uv}r_{vi}
$$

其中，$S(u, K)$包含和用户$u$兴趣最接近的$K$个用户，$N(i)$是对物品$i$有过行为的用户集合，$w_{uv}$是用户$u$和用户$v$的兴趣相似度，$r_{vi}$代表用户$v$对物品$i$x的兴趣.


\subsection{基于物品的协同过滤}
UserCF在一些网站(如Digg)中得到了应用，但该算法有一些缺点。首先， 随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。其次，基于用户的协同过滤很难对推荐结果作出解释。因此，著名的电子商务公司亚马逊提出了另一个算法——基于物品的协同过滤算法。


基于物品的协同过滤算法(简称ItemCF)给用户推荐那些和他们之前喜欢的物品相似的物品。 比如，该算法会因为你购买过《数据挖掘导论》而给你推荐《机器学习》。不过，\textbf{ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品 B}。

基于物品的协同过滤算法主要分为两步：(1) 计算物品之间的相似度；(2)根据物品的相似度和用户的历史行为给用户生成召回候选集。

ItemCF的第一步是计算物品之间的相似度，在网站中，我们经常看到这么一句话：Customers Who Bought This Item Also Bought，那么从这句话的定义出发，我们可以用下面的公式定义物品相似度：
$$
w_{ij} = \frac{|N(i) \cap N(j)|}{|N(i)|}
$$

这里，分母$|N(i)|$是喜欢物品$i$的用户数，而分子 $N(i)\cap N(j) $是同时喜欢物品$i$和物品$j$的用户数。因此，上述公式可以理解为喜欢物品$i$的用户中有多少比例的用户也喜欢物品$j$。但是却存在一个问题。如果物品$j$很热门，很多人都喜欢，那么$W_{ij}$就会很大，接近1。因此，该公式会造成任何物品都会和热门的物品有很大的相似度，这 对于致力于挖掘长尾信息的推荐系统来说显然不是一个好的特性。为了避免推荐出热门的物品，可以用下面的公式:
$$
w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)||N(j)|}}
$$

这里由于还是0-1的原因，我们的余弦相似度可以写成上面的形式。但是，是不是每个用户的贡献都相同呢? 假设有这么一个用户，他是开书店的，并且买了当当网上80\%的书准备用来自己卖。那么， 他的购物车里包含当当网80\%的书。假设当当网有100万本书，也就是说他买了80万本。从前面 对ItemCF的讨论可以看到，这意味着因为存在这么一个用户，有80万本书两两之间就产生了相似度。这个用户虽然活跃，但是买这些书并非都是出于自身的兴趣，而且这些书覆 盖了当当网图书的很多领域，所以这个用户对于他所购买书的两两相似度的贡献应该远远小于一个只买了十几本自己喜欢的书的文学青年。因此，我们要对这样的用户进行一定的惩罚，John S. Breese在论文1中提出了一个称为IUF(Inverse User Frequence)，即用户活跃度对数的 倒数的参数，他也认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF参数来修正物品相似度的计算公式:
$$
w_{ij} = \frac{\sum_{u \in N(i) \cap N(j)\frac{1}{\log 1 + |N(u)|}}}{\sqrt{|N(i)||N(j)|}}
$$

在得到物品之间的相似度后，ItemCF通过如下公式计算用户$u$对一个物品j的兴趣:
$$
P_{uj} = \sum_{i \in N(u) \cap S(j,K)}w_{ji}r_{ui}
$$

这里$N(u)$是用户喜欢的物品的集合，$S(j,K)$是和物品$j$最相似的$K$个物品的集合，$w_{ji}$是物品$j$和$i$的相似度，$r_{ui}$是用户$u$对物品$i$的兴趣。

\subsection{UserCF和ItemCF的比较}
先说结论：\textbf{新闻网站一般使用UserCF，而图书、电商网站一般使用ItemCF}。

首先回顾一下UserCF算法和ItemCF算法的推荐原理。UserCF给用户推荐那些和他有共同兴 趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算法的原理可以看到，\textbf{UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF 的推荐结果着重于维系用户的历史兴趣}。换句话说，UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。

在新闻网站中，用户的兴趣不是特别细化，绝大多数用户都喜欢看热门的新闻。个性化新闻推荐更加强调抓住 新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。因 此，UserCF可以给用户推荐和他有相似爱好的一群其他用户今天都在看的新闻，这样在抓住热 点和时效性的同时，保证了一定程度的个性化。同时，在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且 对于新用户，完全可以给他推荐最热门的新闻，因此UserCF显然是利大于弊。

但是，在图书、电子商务和电影网站，比如亚马逊、豆瓣、Netflix中，ItemCF则能极大地发 挥优势。首先，在这些网站中，用户的兴趣是比较固定和持久的。一个技术人员可能都是在购买 技术方面的书，而且他们对书的热门程度并不是那么敏感，事实上越是资深的技术人员，他们看的书就越可能不热门。此外，这些系统中的用户大都不太需要流行度来辅助他们判断一个物品的好坏，而是可以通过自己熟悉领域的知识自己判断物品的质量。因此，这些网站中个性化推荐的 任务是帮助用户发现和他研究领域相关的物品。因此，ItemCF算法成为了这些网站的首选算法。 此外，这些网站的物品更新速度不会特别快，一天一次更新物品相似度矩阵对它们来说不会造成太大的损失，是可以接受的。同时，从技术上考虑，UserCF需要维护一个用户相似度的矩阵，而ItemCF需要维护一个物品相似度矩阵。从存储的角度说，如果用户很多，那么维护用户兴趣相似度矩阵需要很大的空间， 同理，如果物品很多，那么维护物品相似度矩阵代价较大。

\subsection{协同过滤算法的问题}
热门的商品具有很强的头部效应，容易跟大量物品产生相似性；而尾部的物品由于特征向量稀疏，很少与其他物品产生相似性，导致很少被推荐。

\section{矩阵分解算法——协同过滤的进化}

\section{深度树匹配}
\url{https://www.jianshu.com/p/ef3caa5672c8}

\section{LR——CTR模型的核心和基础}
位于正中央的是当之无愧的Logistic Regression。仍记得2012年我刚进入计算广告这个行业的时候，各大中小公司的主流CTR模型无一例外全都是LR模型。LR模型的流行是有三方面原因的，一是数学形式和含义上的支撑；二是人类的直觉和可解释性的原因；三是工程化的需要。

\subsection{逻辑回归的数学基础}
逻辑回归作为广义线性模型的一种，\textbf{它的假设是因变量 $y$ 服从伯努利分布}。那么在点击率预估这个问题上，“点击”这个事件是否发生就是模型的因变量 $y$。而用户是否点击广告这个问题是一个经典的掷偏心硬币问题，因此CTR模型的因变量显然应该服从伯努利分布。所以采用LR作为CTR 模型是符合“点击”这一事件的物理意义的。

与之相比较，线性回归（Linear Regression）作为广义线性模型的另一个特例，其\textbf{假设是因变量 $y$ 服从高斯分布}，这明显不是点击这类二分类问题的数学假设。

在了解LR的数学理论基础后，其数学形式就不再是空中楼阁了，具体的形式如下：
$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
$$

其中 $x$ 是输入向量，$\theta$ 是我们要学习的参数向量。结合CTR模型的问题来说，$x$ 就是输入的特征向量，$h(x)$ 就是我们最终希望得到的点击率。

\subsection{LR 的训练方法}
常见训练方法是梯度下降法、牛顿法、拟牛顿法等。

\subsection{人类的直觉和可解释性}
直观来讲，LR模型目标函数的形式就是各特征的加权和，再施以sigmoid函数。忽略其数学基础（虽然这是其模型成立的本质支撑），仅靠人类的直觉认知也可以一定程度上得出使用LR作为CTR模型的合理性。

使用各特征的加权和是为了综合不同特征对CTR的影响，而由于不同特征的重要程度不一样，所以为不同特征指定不同的权重来代表不同特征的重要程度。最后要套上sigmoid函数，正是希望其值能够映射到0-1之间，使其符合CTR的物理意义。

LR如此符合人类的直觉认知显然有其他的好处，就是模型具有极强的可解释性，算法工程师们可以轻易的解释哪些特征比较重要，在CTR模型的预测有偏差的时候，也可以轻易找到哪些因素影响了最后的结果，如果你有跟运营、产品一起工作的经验的话，更会知道可解释性强是一个模型多么优秀的“品质”。

\subsection{工程化的需要}
在互联网公司每天动辄TB级别的数据面前，模型的训练开销就异常重要了。在GPU尚未流行开来的2012年之前，LR模型也凭借其易于并行化、模型简单、训练开销小等特点占据着工程领域的主流。囿于工程团队的限制，即使其他复杂模型的效果有所提升，在没有明显beat LR之前，公司也不会贸然加大计算资源的投入升级CTR模型，这是LR持续流行的另一重要原因。

\section{从 POLY2 到FM/FFM——自动特征交叉的解决方案}
\subsection{POLY2——特征交叉的开始}
但LR的表达能力毕竟是非常初级的。由于LR仅使用单一特征，无法利用高维信息，在“辛普森悖论”现象的存在下，只用单一特征进行判断，甚至会得出错误的结论。

在辛普森悖论中，分组实验相当于使用性别+视频id 的组合特征计算点击率，而汇总实验则使用视频 id 这一单一特征计算点击率。\textbf{汇总实验对高维特征进行了合并，损失了大量的有效信息，因此无法正确刻画数据模式}。

针对这个问题，当时的算法工程师们经常采用手动组合特征，再通过各种分析手段筛选特征的方法。但这个方法无疑是残忍的，完全不符合“懒惰是程序员的美德”这一金科玉律。更遗憾的是，人类的经验往往有局限性，程序员的时间和精力也无法支撑其找到最优的特征组合。因此采用 PLOY2模型进行特征的“暴力”组合成为了可行的选择。
$$
\phi_{\text{Poly2}}(\mathbf{w}, \mathbf{x}) = \sum_{j_1=1}^n\sum_{j_2 = j_1 + 1}^n w_h(j_1, j_2)x_{j_1}x_{j_2}
$$

在上面POLY2二阶部分的目标函数中（上式省略一阶部分和sigmoid函数的部分），我们可以看到POLY2对所有特征进行了两两交叉，并对所有的特征组合赋予了权重 $w_h(j_1, j_2)$。POLY2通过暴力组合特征的方式一定程度上解决了特征组合的问题。并且由于本质上仍是线性模型，其训练方法与LR并无区别，便于工程上的兼容。

但POLY2这一模型同时存在着两个巨大的缺陷：(1) 由于在处理互联网数据时，经常采用one-hot的方法处理id类数据，致使特征向量极度稀疏，POLY2进行无选择的特征交叉使原本就非常稀疏的特征向量更加稀疏，使得大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛；(2) 权重参数的数量由 $n$ 直接上升到 $n^2$，极大增加了训练复杂度。

\subsection{FM——隐向量特征交叉}
为了解决POLY2模型的缺陷，2010年德国康斯坦茨大学的Steffen Rendle提出了FM（Factorization Machine）。
$$
\phi_{\text{FM}}(\mathbf{w}, \mathbf{x}) = \sum_{j_1=1}^n\sum_{j_2 = j_1 + 1}^n (\mathbf{w}_{j_1} \cdot \mathbf{w}_{j_2})x_{j_1}x_{j_2}
$$

从 FM 的目标函数的二阶部分中我们可以看到，相比 POLY2，主要区别是用两个向量的内积 $\mathbf{w}_{j_1} \cdot \mathbf{w}_{j_2}$取代了单一的权重 $w_h(j_1, j_2)$。具体来说，FM为每个特征学习了一个隐权重向量（latent vector），在特征交叉时，使用两个特征隐向量的内积作为交叉特征的权重。

通过引入特征隐向量的方式，直接把原先 $n^2$ 级别的权重数量减低到了 $n*k$（$k$为隐向量维度，$n >> k$）。在训练过程中，又可以通过转换目标函数形式的方法，使 FM 的训练复杂度进一步降低到 $n*k$ 级别。相比POLY2极大降低训练开销。

\textbf{隐向量的引入还使得FM比POLY2能够更好的解决数据稀疏性的问题}。举例来说，我们有两个特征，分别是channel和brand，一个训练样本的feature组合是(ESPN, Adidas)，在POLY2中，只有当ESPN和Adidas同时出现在一个训练样本中时，模型才能学到这个组合特征对应的权重。而在FM中，ESPN的隐向量也可以通过(ESPN, Gucci)这个样本学到，Adidas的隐向量也可以通过(NBC, Adidas)学到，这大大降低了模型对于数据稀疏性的要求。甚至对于一个从未出现过的特征组合(NBC, Gucci)，由于模型之前已经分别学习过NBC和Gucci的隐向量，FM也具备了计算该特征组合权重的能力，这是POLY2无法实现的。也许FM相比POLY2丢失了某些信息的记忆能力，但是泛化能力大大提高，这对于互联网的数据特点是非常重要的。

工程方面，FM同样可以用梯度下降进行学习的特点使其不失实时性和灵活性。相比之后深度学习模型复杂的网络结构，FM比较容易实现的inference过程也使其没有serving的难题。因此FM在2012-2014年前后逐渐成为业界CTR模型的重要选择。

\subsection{FFM——引入特征域概念}
2015年，基于FM提出的FFM（Field-aware Factorization Machine ，简称FFM）在多项CTR预估大赛中一举夺魁，并随后被Criteo、美团等公司深度应用在CTR预估，推荐系统领域。相比FM模型，FFM模型主要引入了Field-aware这一概念，使模型的表达能力更强。
$$
\phi_{\text{FFM}}(\mathbf{w}, \mathbf{x}) = \sum_{j_1=1}^n\sum_{j_2 = j_1 + 1}^n (\mathbf{w}_{j_1, f_2} \cdot \mathbf{w}_{j_2, f_1})x_{j_1}x_{j_2}
$$

上式是 FFM 的目标函数的二阶部分。其与 FM 目标函数的区别就在于隐向量由原来的 $w_{j_1}$ 变成了 $w_{j_1, f_2}$  ，这就意味着每个特征对应的不是一个隐向量，而是对应着不同域的一组隐向量，当 $w_{j_1}$ 特征与 $w_{j_2}$ 特征进行交叉时，$x_{j_1}$ 特征会从一组隐向量中挑出与特征 $2_{j_2}$ 的域 $f_2$ 对应的隐向量 $w_{j_1, f_2}$ 进行交叉。同理特征 $x_{j_2}$ 也会用与 $x_{j_1}$ 的域 $f_1$ 对应的隐向量进行交叉。

这里再次强调一下，上面所说的“域”就代表着特征域，域内的特征一般会采用one-hot编码形成one-hot特征向量。

FFM模型学习每个特征在 $f$ 个域上的 $k$ 维隐向量，交叉特征的权重由特征在对方特征域上的隐向量内积得到，权重数量共 $n*k*f$ 个。在训练方面，由于FFM的二次项并不能够像 FM 那样简化，因此其复杂度为 $kn^2$。

相比 FM，FFM 由于引入了 field 这一概念，为模型引入了更多有价值信息，使模型表达能力更强，但与此同时，FFM的计算复杂度上升到$kn^2$，远远大于FM的 $k*n$。

\begin{framed}
理解：比较 Poly2、 FM、FFM

Poly2：对所有特征进行两两交叉，为每种特征组合都学习一个权重；

FM：为每个特征学习一个隐权重向量。在特征交叉时，使用两个特征隐向量的内积作为交叉特征的权重。

FFM：为每个特征学习一个隐权重“矩阵”，矩阵的每一列对应一个域，该列向量为对应的隐权重向量。在特征交叉时，使用两个特征对应域的隐向量的内积作为交叉特征的权重。
\end{framed}

\subsection{CTR模型特征交叉方向的演化}
以上模型实际上是CTR模型朝着特征交叉的方向演化的过程，我们再用图示方法回顾一下从POLY2到FM，再到FFM进行特征交叉方法的不同。

POLY2 模型直接学习每个交叉特征的权重，权重数量共 $n^2$ 个。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/CTR_Comparison_Poly2.png}
\end{figure}

FM 模型学习每个特征的 $k$ 维隐向量，交叉特征由相应特征隐向量的内积得到，权重数量共 $n*k$ 个。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/CTR_Comparison_FM.png}
\end{figure}

FFM 模型引入了特征域这一概念，在做特征交叉时，每个特征选择与对方域对应的隐向量做内积运算得到交叉特征的权重。参数数量共 $n*k*f$个。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/CTR_Comparison_FFM.png}
\end{figure}

\section{GBDT+LR——特征工程模型化的开端}
FFM模型采用引入特征域的方式增强了模型的表达能力，但无论如何，FFM只能够做二阶的特征交叉，如果要继续提高特征交叉的维度，不可避免的会发生组合爆炸和计算复杂度过高的情况。那么有没有其他的方法可以有效的处理高维特征组合和筛选的问题？2014年，Facebook提出了基于GBDT+LR组合模型的解决方案。

\subsection{GBDT+LR组合模型的结构}
简而言之，Facebook提出了一种利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入，预估CTR的模型结构。

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_GBDT_LR_Feature.jpg}
\end{figure}

大家知道，GBDT 是由多棵回归树组成的树林，后一棵树利用前面树林的结果与真实结果的残差做为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构自然进行了有效的特征组合，也就非常高效的解决了过去非常棘手的特征选择和特征组合的问题。

需要强调的是，用 GBDT 构建特征工程，利用 LR 预估 CTR 这两步是独立训练的，所以不存在如何将 LR 的梯度回传到 GBDT 这类复杂的问题。

\begin{framed}
GBDT 模型简介：

GBDT 的基本结构是决策树组成的树林，学习的方式是梯度提升。

具体来说，GBDT 作为集成模型，预测的方式是把所有子树的结果加起来：
$$
D(x) = d_{\text{tree}_1(x)} + d_{\text{tree}_2(x)} + \cdots
$$

GBDT 通过逐一生成决策子树的方式生成整个树林，生成新子树的过程是利用样本标签值与当前树林预测值之间的残差，构建新的子树。

假设当前已经生成了 3 棵子树，则当前的预测值为：
$$
D(x) = d_{\text{tree}_1(x)} + d_{\text{tree}_2(x)} + d_{\text{tree}_3(x)}
$$

GBDT 期望的是构建第 4 棵子树，使当前树林的预测结果 $D(x)$ 与第 4 棵子树的预测结果 $d_{\text{tree}_4(x)}$ 之和，能进一步逼近理论上的拟合函数 $f(x)$，即：
$$
D(x) + d_{\text{tree}_4(x)} = f(x)
$$

所以第 4 棵子树生成的过程是以目标拟合函数和已有树林预测结果的残差 $R(x)$ 为目标的：
$$
R(x) = f(x) - D(x)
$$

理论上，如果可以无限生成决策树，那么 GBDT 可以无限逼近由所有训练集样本组成的目标拟合函数，从而达到减小预测误差的目的。
\end{framed}

\subsection{GBDT 进行特征转换的过程}

利用训练集训练好 GBDT 模型之后，就可以利用该模型完成从原始特征向量到新的离散型特征向量的转化。具体过程是这样的，一个训练样本在输入 GBDT 的某一子树后，会根据每个节点的规则最终落入某一叶子节点，那么我们把该叶子节点置为1，其他叶子节点置为0，所有叶子节点组成的向量即形成了该棵树的特征向量，把GBDT所有子树的特征向量连接起来，即形成了后续LR输入的特征向量。

由于决策树的结构特点，事实上，\textbf{决策树的深度就决定了特征交叉的维度}。如果决策树的深度为4，通过三次节点分裂，最终的叶节点实际上是进行了3阶特征组合后的结果，如此强的特征组合能力显然是FM系的模型不具备的。但由于GBDT容易产生过拟合，以及GBDT这种特征转换方式实际上丢失了大量特征的数值信息，因此我们不能简单说GBDT由于特征交叉的能力更强，效果就比FFM好，在模型的选择和调试上，永远都是多种因素综合作用的结果。

GBDT+LR比FM重要的意义在于，\textbf{它大大推进了特征工程模型化这一重要趋势}，某种意义上来说，之后深度学习的各类网络结构，以及embedding技术的应用，都是这一趋势的延续。

\section{FTRL——天下武功，唯快不破}
\subsection{FTRL 简介}
FTRL的全称是Follow-the-regularized-Leader，是一种在线实时训练模型的方法，Google在2010年提出了FTRL的思路，2013年实现了FTRL的工程化，之后快速成为online learning的主流方法。与模型演化图中的其他模型不同，FTRL本质上是模型的训练方法。虽然Google的工程化方案是针对LR模型的，但理论上FTRL可以应用在FM，NN等任何通过梯度下降训练的模型上。

\subsection{Online Learning 下的稀疏性问题和解法}
为了更清楚的认识FTRL，这里对梯度下降方法做一个简要的介绍，从训练样本的规模角度来说，梯度下降可以分为：batch，mini-batch，SGD（随机梯度下降）三种，batch方法每次都使用全量训练样本计算本次迭代的梯度方向，mini-batch使用一小部分样本进行迭代，而SGD每次只利用一个样本计算梯度。对于online learning来说，为了进行实时得将最新产生的样本反馈到模型中，SGD无疑是最合适的训练方式。

但\textbf{SGD对于互利网广告和推荐的场景来说，有比较大的缺陷，就是难以产生稀疏解}。为什么稀疏解对于CTR模型如此重要呢？

之前我们已经多次强调，由于one hot等id类特征处理方法导致广告和推荐场景下的样本特征向量极度稀疏，维度极高，动辄达到百万、千万量级。为了不割裂特征选择和模型训练两个步骤，如果能够在保证精度的前提下尽可能多的让模型的参数权重为0，那么我们就可以自动过滤掉这些权重为0的特征，生成一个“轻量级”的模型。“轻量级”的模型不仅会使样本部署的成本大大降低，而且可以极大降低模型inference的计算延迟。这就是模型稀疏性的重要之处。

而SGD由于每次迭代只选取一个样本，梯度下降的方向虽然总体朝向全局最优解，但微观上的运动的过程呈现布朗运动的形式，这就导致SGD会使几乎所有特征的权重非零。即使加入L1正则化项，由于CPU浮点运算的结果很难精确的得到0的结果，也不会完全解决SGD稀疏性差的问题。就是在这样的前提下，\textbf{FTRL几乎完美地解决了模型精度和模型稀疏性兼顾的训练问题}。

\subsection{FTRL 的演化过程}
但FTRL的提出也并不是一蹴而就的。如下图所示，FTRL的提出经历了下面几个关键的过程：
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/FTRL_Evolution.jpg}
\end{figure}

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textbf{从最近简单的SGD到OGD（online gradient descent）}，OGD通过引入L1正则化简单解决稀疏性问题；
    \item \textbf{从OGD到截断梯度法}，通过暴力截断小数值梯度的方法保证模型的稀疏性，但损失了梯度下降的效率和精度；
    \item \textbf{FOBOS（Forward-Backward Splitting）}，google和伯克利对OGD做进一步改进，09年提出了保证精度并兼顾稀疏性的FOBOS方法；
    \item \textbf{RDA}：微软抛弃了梯度下降这条路，独辟蹊径提出了正则对偶平均来进行online learning的方法，其特点是稀疏性极佳，但损失了部分精度。
    \item Google\textbf{综合FOBOS在精度上的优势和RDA在稀疏性上的优势}，将二者的形式进行了进一步统一，提出并应用FTRL，使FOBOS和RDA均成为了FTRL在特定条件下的特殊形式。
\end{itemize}

\section{LS-PLM(MLR)——阿里曾经的主流CTR模型}
下面我们从样本pattern本身来入手，介绍阿里的的LS-PLM（Large Scale Piece-wise Linear Model），它的另一个更广为人知的名字是MLR（Mixed Logistic Regression）。MLR模型虽然在2017年才公之于众，但其早在2012年就是阿里主流的CTR模型，并且在深度学习模型提出之前长时间应用于阿里的各类广告场景。

\subsection{LS-PLM 模型的主要结构}
本质上，MLR可以看做是对LR的自然推广，它在LR的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用LR进行CTR预估。在LR的基础上加入聚类的思想，其动机其实来源于对计算广告领域样本特点的观察 。

MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，其形式化表达如下\cite{Recommender_System_With_Deep_Learning_MLR}：
$$
p(y=1|x) = g\Bigg( \sum_{j=1}^m\sigma\Big(u^T_jx\Big)\eta\Big(w^T_jx\Big)\Bigg)
$$

其中$u$是聚类参数，决定了空间的划分，$w$是分类参数，决定空间内的预测。这里面超参数分片数$m$可以较好地平衡模型的拟合与推广能力。当$m=1$时MLR就退化为普通的LR，$m$越大模型的拟合能力越强，但是模型参数规模随$m$线性增长，相应所需的训练样本也随之增长。因此实际应用中m需要根据实际情况进行选择。


举例来说，如果CTR模型要预估的是女性受众点击女装广告的CTR，显然我们并不希望把男性用户点击数码类产品的样本数据也考虑进来，因为这样的样本不仅对于女性购买女装这样的广告场景毫无相关性，甚至会在模型训练过程中扰乱相关特征的权重。为了让CTR模型对不同用户群体，不用用户场景更有针对性，其实理想的方法是\textbf{先对全量样本进行聚类，再对每个分类施以LR模型进行CTR预估}。MLR的实现思路就是由该动机产生的。
$$
f(x) = \sum_{i=1}^m\pi_i(x) \cdot \eta_i(x) = \sum_{i=1}^m \frac{e^{\mu_i \cdot x}}{\sum_{j=1}^m e^{\mu_j\cdot x}} \cdot \frac{1}{1+e^{-w_i\cdot x}}
$$

MLR目标函数的数学形式如上式，首先用聚类函数 $\pi$ 对样本进行分类（这里的 $\pi$ 采用了softmax函数，对样本进行多分类），再用 LR模型计算样本在分片中具体的 CTR，然后将二者进行相乘后加和。

其中超参数分片数 $m$ 可以较好地平衡模型的拟合与推广能力。当 $m=1$ 时MLR就退化为普通的LR，$m$ 越大模型的拟合能力越强，但是模型参数规模随 $m$ 线性增长，相应所需的训练样本也随之增长。在实践中，阿里给出了 $m$ 的经验值为12。

在这种情况下，MLR模型可以看作是一个FOE model：
$$
p(y=1|x) = \sum_{i=1}^mp(z=i|x)p(y|z=i,x)
$$

关于损失函数的设计，阿里采用了 neg-likelihood loss function以及L1，L2正则，形式如下：
$$
\arg\min_\Theta f(\Theta) = \text{loss}(\Theta) + \lambda||\Theta||_{2,1} + \beta||\Theta||_1
$$
$$
\text{loss}(\Theta) = -\sum_{t=1}^n\Bigg[y_t\log\Big(p(y_t=1|x_t,\Theta)\Big) + \Big(1-y_t\Big)\log\Big(p(y_t=0|x_t,\Theta)\Big)\Bigg]
$$

由于加入了正则项，MLR算法变的不再是平滑的凸函数，梯度下降法不再适用，因此模型参数的更新使用LBFGS和OWLQN的结合，具体的优化细节大家可以参考论文(\url{https://arxiv.org/pdf/1704.05194.pdf})。


\subsection{LS-PLM 模型的特点}
MLR算法适合于工业级的广告、推荐等大规模稀疏数据场景问题。主要是由于表达能力强、稀疏性高等两个优势。

(1). \textbf{端到端的非线性学习}：从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。

(2). \textbf{稀疏性}：MLR 在建模时引入了L1和L2,1范数，可以使得最终训练出来的模型具有较高的稀疏度，模型的学习和在线预测性能更好。

\subsection{从深度学习的角度重新审视 LS-PLM 模型}
如果我们用深度学习的眼光来看待MLR这个模型，其在结构上已经很接近由输入层、单隐层、输出层组成的神经网络。所以某种意义上说，MLR也在用自己的方式逐渐逼近深度学习的大门了。

LS-PLM 模型可以看做一个加入了注意力(Attention)机制的三层神经网络模型，其中输入层是样本的特征向量；中间层是由 $m$ 个神经元组成的隐层，其中 $m$ 是分片的个数；对于一个 CTR 预估问题而，LS-PLM 的最后一层是由单一神经元组成的输出层。

那么，注意力机制又是在哪里应用的呢？其实是在隐层和输出层之间，神经元之前的权重是由分片函数得出的注意力得分来确定的。也就是说，样本属于哪个分片的概率就是其注意力得分。

\section{总结——深度学习CTR模型的前夜}
2010年FM被提出，特征交叉的概念被引入CTR模型；2012年MLR在阿里大规模应用，其结构十分接近三层神经网络；2014年Facebook用GBDT处理特征，揭开了特征工程模型化的篇章。这些概念都将在深度学习CTR模型中继续应用，持续发光。

另一边，Alex Krizhevsky 2012年提出了引爆整个深度学习浪潮的AlexNet，深度学习的大幕正式拉开，其应用逐渐从图像扩展到语音，再到NLP领域，推荐和广告也必然会紧随其后，投入深度学习的大潮之中。

2016年，随着FNN，Deep\&Wide，Deep crossing等一大批优秀的CTR模型框架的提出，深度学习CTR模型逐渐席卷了推荐和广告领域，成为新一代CTR模型当之无愧的主流。

\part{深度学习推荐模型的演化趋势}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Deep_Learning_Reference_System_Evolution.jpg}
\end{figure}

上图中涉及到的主要知识点如下：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item Product 层
    \item AutoEncoder
    \item Attention 机制
    \item ARGRU
    \item Cross Layer Network
    \item Bi-interaction
    \item ……
\end{itemize}

\section{AutoRec——单隐层神经网络推荐模型}

\section{微软Deep Crossing（2016年）——深度学习CTR模型的base model}
论文：[Deep Crossing] Deep Crossing - Web-Scale Modeling without Manually Crafted Combinatorial Features (Microsoft 2016)

\subsection{DeepCrossing 模型的应用场景}
微软于2016年提出的Deep Crossing可以说是深度学习CTR模型的最典型和基础性的模型。如图1的模型结构图所示，它涵盖了深度CTR模型最典型的要素，即通过加入embedding层将稀疏特征转化为低维稠密特征，用stacking layer，或者叫做concat layer将分段的特征向量连接起来，再通过多层神经网络完成特征的组合、转换，最终用scoring layer完成CTR的计算。跟经典DNN有所不同的是，Deep crossing采用的multilayer perceptron是由残差网络组成的，这无疑得益于MSRA著名研究员何恺明提出的著名的152层ResNet。

DeepCrossing 模型的应用场景是微软搜索引擎 Bing 中的搜索广告推荐场景，用户在搜索引擎中输入搜索词之后，搜索引擎除了会返回相关结果，还会返回与搜索词相关的广告。

DeepCrossing 使用的特征包括：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item one-hot/multi-hot 类特征：用户搜索词(query)、广告关键词（keyword）、广告标题(title)、落地页(landing page)、匹配类型(match type)等；
    \item 数值类特征（计数型特征 counting feature）：点击率、预估点击率等；
    \item 需要进一步处理的特征：广告计划(campaign)、曝光样例(impression)、点击样例(click)等；
\end{itemize}

\subsection{DeepCrossing 模型的网络结构}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Microsoft_Deep_Crossing_Strucure.png}
\end{figure}

Embedding 层：将稀疏的类别性特征转换为稠密的 Embedding 向量。

Stacking 层：作用比较简单，是把不同的 Embedding 向量和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也被成为连接（concatenate ）层。

Multiple Residual Units 层：该层的主要结构是多层感知机，采用了多层残差网络（Multi-Layer Residual Network）作为 MLP 的具体实现。

Scoring 层：该层为输出层，为了拟合优化目标而存在。对于 CTR 这类二分类问题，往往采用逻辑回归模型，对于多分类问题，通常采用 softmax 模型。

\section{NeuralCF 模型——CF 与深度学习的结合}
\url{https://www.jianshu.com/p/6173dbde4f53}


\section{PNN (2016年)——丰富特征交叉的方式}
\subsection{PNN简介}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/PNN_Structure.jpg}
\end{figure}

PNN的全称是Product-based Neural Network，\textbf{PNN的关键在于在embedding层和全连接层之间加入了Product layer}。传统的DNN是直接通过多层全连接层完成特征的交叉和组合的，但这样的方式缺乏一定的“针对性”。首先全连接层并没有针对不同特征域之间进行交叉；其次，全连接层的操作也并不是直接针对特征交叉设计的。但在实际问题中，特征交叉的重要性不言而喻，比如年龄与性别的交叉是非常重要的分组特征，包含了大量高价值的信息，我们急需深度学习网络能够有针对性的结构能够表征这些信息。因此PNN通过加入Product layer完成了针对性的特征交叉，其product操作在不同特征域之间进行特征组合。并定义了inner product，outer product等多种product的操作捕捉不同的交叉信息，增强模型表征不同数据模式的能力 。

论文：[PNN] Product-based Neural Networks for User Response Prediction (SJTU 2016)

\subsection{模型细节}
\subsubsection{输出层}
输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：
$$
\hat{y} = \sigma(W_3l_2 + b_3)
$$

\subsubsection{l2层}
根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：
$$
l_2 = \text{relu}(W_2l_1 + b_2)
$$

\subsubsection{l1层}
l1层的输出由如下的公式计算：
$$
l_1 = \text{relu}(l_z + l_p + b_1)
$$

重点马上就要来了，我们可以看到在得到l1层输出时，我们输入了三部分，分别是$l_z$，$l_p$ 和 $b_1$，$b_1$ 是我们的偏置项，这里可以先不管。$l_z$ 和 $l_p$ 的计算就是PNN的精华所在了。

\subsubsection{Product Layer}
product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add"加”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。

product layer可以分成两个部分，一部分是线性部分$l_z$，一部分是非线性部分$l_p$。二者的形式如下：
$$
l_z = (l_z^1, l_z^2, \cdots, l_z^n, \cdots, l_z^{D1}) \qquad \qquad l_z^n   = W_z^n \odot z
$$
$$
l_p = (l_p^1, l_p^2, \cdots, l_p^n, \cdots, l_p^{D1}) \qquad \qquad l_p^n   = W_p^n \odot p
$$

在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘：
$$
A \odot B = \sum A_{i,j}B_{i,j}
$$

\subsubsection{Embedding Layer}
Embedding Layer 跟 DeepFM 中相同，将每一个 field 的特征转换成同样长度的向量，这里用 $f$ 来表示。
$$
(f_1, f_2, \cdots, f_N)
$$

\subsubsection{损失函数}
使用和逻辑回归同样的损失函数，如下：
$$
L(y, \hat{y}) = -y \log \hat{y} - (1-y)\log (1-\hat{y})
$$

\subsection{Product Layer详细介绍}
前面提到了，product layer可以分成两个部分，一部分是线性部分$l_z$，一部分是非线性部分$l_p$。

看上面的公式，我们首先需要知道$z$和$p$，这都是由我们的embedding层得到的，其中$z$是线性信号向量，因此我们直接用embedding层得到：
$$
z = (z_1, z_2, \cdots, z_n) \triangleq (f_1, f_2, \cdots, f_N)
$$

论文中使用的等号加一个三角形，其实就是相等的意思，你可以认为$z$就是embedding层的复制。

对于$p$来说，这里需要一个公式进行映射：
$$
p = \{p_{i,j}\}, i = 1, \cdots, N, j = 1, \cdots, N
$$
$$
p_{i,j} = g(f_i, f_j)
$$

不同的 $g$ 的选择使得我们有了两种 PNN 的计算方法，一种叫做Inner PNN，简称 IPNN，一种叫做 Outer PNN，简称 OPNN。

接下来，我们分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义 Embedding 的大小为 $M$，field 的大小为 $N$，而 $l_z$ 和 $l_p$ 的长度为 $D_1$：

\subsubsection{IPNN}
IPNN的示意图如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.3\textwidth]{fig/DCN_IPNN_Example.png}
\end{figure}

IPNN中 $p$ 的计算方式如下，即使用内积来代表 $p_{ij}$：
$$
g(f_i, f_j) = \ < f_i, f_j >
$$

所以，$p_{ij}$其实是一个数，得到一个 $p_{ij}$ 的时间复杂度为 $M$，$p$ 的大小为 $N*N$，因此计算得到 $p$ 的时间复杂度为$N*N*M$。而再由 $p$ 得到 $l_p$的时间复杂度是 $N*N*D_1$。因此 对于 IPNN 来说，总的时间复杂度为 $N*N(D_1+M)$。文章对这一结构进行了优化，可以看到，我们的 $p$ 是一个对称矩阵，因此我们的权重也可以是一个对称矩阵，对称矩阵就可以进行如下的分解：
$$
W_p^n = \theta^n \theta^{nT}
$$

因此：
$$
l_p^n = W_p^n \odot p = \sum_{i=1}^N\sum_{j=1}^N \theta_i^n \theta_j^n <f_i, f_j> = < \sum_{i=1}^N\delta_i^n, \sum_{i=1}^N\delta_i^n >
$$
$$
\delta_i^n = \theta_i^nf_i
$$

因此：
$$
\delta^n = (\delta_1^n, \delta_2^n, \cdots, \delta_i^n, \cdots, \delta_N^n) \in \mathbb{R}^{N\times M}
$$

从而得到：
$$
l_p = \Big( ||\sum_i\delta_i^1||, \cdots,  ||\sum_i\delta_i^n||, \cdots,  ||\sum_i\delta_i^{D_1}||\Big)
$$

可以看到，我们的权重只需要$D_1 * N$就可以了，时间复杂度也变为了$D_1*M*N$。

\subsubsection{OPNN}
OPNN的示意图如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.3\textwidth]{fig/DCN_OPNN_Example.png}
\end{figure}

OPNN中 $p$ 的计算方式如下：
$$
p_{i,j} = g(f_i, f_j) = f_if_j^T
$$

此时 $p_{ij}$ 为 $M*M$ 的矩阵，计算一个 $p_{ij}$ 的时间复杂度为$M*M$，而 $p$ 是 $N*N*M*M$ 的矩阵，因此计算 $p$ 的时间复杂度为 $N*N*M*M$。从而计算 $l_p$ 的时间复杂度变为 $D1 * N*N*M*M$。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了 $p$ 矩阵：
$$
p = \sum_{i=1}^N\sum_{j=1}^Nf_if_j^T = f_{\sum}(f_{\sum})^T, \quad f_{\sum} = \sum_{i=1}^Nf_i
$$

这里计算 $p$ 的时间复杂度变为了 $D1*M*(M+N)$。

\section{Google Wide\&Deep（2016年）——记忆能力和泛化能力的综合权衡}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Wide_Deep_Structure.png}
\end{figure}

Google Wide\&Deep模型的主要思路正如其名，把单输入层的Wide部分和经过多层感知机的Deep部分连接起来，一起输入最终的输出层。其中Wide部分的主要作用是让模型具有记忆性（Memorization），单层的Wide部分善于处理大量稀疏的id类特征，便于让模型直接“记住”用户的大量历史信息；Deep部分的主要作用是让模型具有“泛化性”（Generalization），利用DNN表达能力强的特点，挖掘藏在特征后面的数据模式。最终利用LR输出层将Wide部分和Deep部分组合起来，形成统一的模型。Wide\&Deep对之后模型的影响在于——大量深度学习模型采用了两部分甚至多部分组合的形式，利用不同网络结构挖掘不同的信息后进行组合，充分利用和结合了不同网络结构的特点。

论文：[Wide\&Deep] Wide \& Deep Learning for Recommender Systems (Google 2016)

\section{FM与深度学习模型的结合}
\subsection{并行结构与串行结构}
在CTR预估中，为了解决稀疏特征的问题，学者们提出了FM模型来建模特征之间的交互关系。但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系或者说多个特征之间的交互关系，因此学者们通过Deep Network来建模更高阶的特征之间的关系\cite{Recommender_System_With_Deep_Learning_NFM}。

因此 FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline 
       \textbf{结构}	    & \textbf{描述}	  & \textbf{常见模型} \\ \hline
       并行结构  & FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果	& DeepFM，DCN，Wide\&Deep \\ \hline
       串行结构	& 将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果	 & PNN,NFM,AFM \\ \hline
    \end{tabular}
\end{table}

\subsection{FNN（2016年）——用FM的隐向量完成Embedding初始化}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/FNN_Structure.png}
\end{figure}

FNN 相比 Deep Crossing 的创新在于\textbf{使用 FM 的隐层向量作为 user 和 item 的 Embedding}，从而避免了完全从随机状态训练Embedding。由于 id 类特征大量采用 one-hot 的编码方式，导致其维度极大，向量极稀疏，所以 Embedding 层与输入层的连接极多，梯度下降的效率很低，这大大增加了模型的训练时间和 Embedding 的不稳定性，使用 pre-train 的方法完成 Embedding 层的训练，无疑是降低深度学习模型复杂度和训练不稳定性的有效工程经验。

论文：[FNN] Deep Learning over Multi-field Categorical Data (UCL 2016)




\subsection{华为 DeepFM (2017年)——用FM代替Wide部分}
\subsubsection{DeepFM 简介}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Huawei_DeepFM_Structure.png}
\end{figure}

在Wide\&Deep之后，诸多模型延续了双网络组合的结构，DeepFM就是其中之一。DeepFM对Wide\&Deep的改进之处在于，它用FM替换掉了原来的Wide部分，加强了浅层网络部分特征组合的能力。事实上，由于FM本身就是由一阶部分和二阶部分组成的，DeepFM相当于同时组合了原Wide部分+二阶特征交叉部分+Deep部分三种结构，无疑进一步增强了模型的表达能力。

论文：[DeepFM] A Factorization-Machine based Neural Network for CTR Prediction (HIT-Huawei 2017)

\subsubsection{DNN的局限和解决思路\cite{Recommender_System_With_Deep_Learning_DeepFM}}
我们之前也介绍过了，对于离散特征的处理，我们使用的是将特征转换成为one-hot的形式，但是将One-hot类型的特征输入到DNN中，会导致网络参数太多；如何解决这个问题呢，类似于FFM中的思想，将特征分为不同的field：
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector.png}
    \caption*{解决思路：从 OneHot 到 DenseVector}
\end{figure}

再加两层的全链接层，让Dense Vector进行组合，那么高阶特征的组合就出来了：
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector_Add_MLP.png}
\end{figure}

\subsubsection{DNN+FM}
但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector_Improve.png}
\end{figure}

即将DNN与FM进行一个合理的融合：
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector_DNN_FM.png}
\end{figure}

二者的融合总的来说有两种形式，一是并行结构，二是串行结构。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector_Fusion_1.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Deep_FM_From_One_Hot_To_Dense_Vector_Fusion_2.png}
\end{figure}

而我们今天要讲到的DeepFM，就是并行结构中的一种典型代表。

\subsubsection{DeepFM模型结构}
观察章节开头 DeepFM 的结构图，可以看出 DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两部分共享同样的输入。DeepFM的预测结果可以写为：
$$
\hat{y} = \text{sigmoid}(y_{FM} + y_{\text{dnn}})
$$

\text{FM部分}

FM部分的详细结构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/DeepFM_FM_Detail.png}
\end{figure}

FM部分是一个因子分解机。因为引入了隐变量的原因，对于几乎不出现或者很少出现的隐变量，FM也可以很好的学习。

\text{深度部分}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/DeepFM_Deep_Detail.png}
\end{figure}

深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/DeepFM_Deep_Detail_Embedding.png}
\end{figure}

嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性，1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2)在FM里得到的隐变量Vik现在作为了嵌入层网络的权重。

这里的第二点如何理解呢，假设我们的k=5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。


\subsection{NFM（2017年）——FM的神经网络化尝试}
\subsubsection{NFM 简介}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/NFM_Structure.jpg}
\end{figure}

NFM 是串行结构中一种较为简单的网络模型。

\subsubsection{NFM 模型结构\cite{Recommender_System_With_Deep_Learning_NFM}}
我们首先来回顾一下 FM 模型，FM 模型用 $n$ 个隐变量来刻画特征之间的交互关系。这里要强调的一点是，$n$ 是特征的总数，是 one-hot 展开之后的，比如有三组特征，两个连续特征，一个离散特征有5个取值，那么 $n=7$ 而不是 $n=3$。
$$
\hat{y}_{\text{FM}(x)} = w_0 + \sum_{i=1}^nw_ix_i + \sum_{i=1}^n\sum_{j=i+1}^nv_i^Tv_j \cdot x_ix_j  
$$

对于 NFM 模型，目标值的预测公式变为：
$$
\hat{y}_{\text{NFM}(x)} = w_0 + \sum_{i=1}^nw_ix_i + f(x)
$$

其中，$f(x)$ 是用来建模特征之间交互关系的多层前馈神经网络模块，架构图如下所示：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/NFM_FM_Detail.png}
\end{figure}

Embedding Layer 和我们之间几个网络是一样的，embedding 得到的vector 其实就是我们在 FM 中要学习的隐变量 $v$。

Bi-Interaction Layer名字挺高大上的，其实它就是计算 FM 中的二次项的过程，因此得到的向量维度就是我们的Embedding的维度。最终的结果是：
$$
f_{BI} (V_x) = \frac{1}{2}\Big[ \Big( \sum_{i=1}^n x_iv_i\Big)^2 - \sum_{i=1}^n(x_iv_i)^2\Big]
$$

Hidden Layers就是我们的DNN部分，将Bi-Interaction Layer得到的结果接入多层的神经网络进行训练，从而捕捉到特征之间复杂的非线性关系。

在进行多层训练之后，将最后一层的输出求和同时加上一次项和偏置项，就得到了我们的预测输出：
$$
\hat{y}_{\text{NFM}(x)} = w_0 + \sum_{i=1}^nw_ix_i + h^T\sigma_L(W_L(\cdots \sigma_1(W_1f_{BI}(V_x)+b_1)\cdots)+b_L)
$$



\subsection{基于 FM 的深度学习模型的优点和局限性}

\section{Google Deep\&Cross（2017年）——使用Cross网络代替Wide部分}
\subsection{DCN简介}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Google_Deep_Cross_Network_Structure.jpg}
\end{figure}

Google 2017年发表的Deep\&Cross Network（DCN）同样是对Wide\&Deep的进一步改进，主要的思路使用Cross网络替代了原来的Wide部分。其中设计Cross网络的基本动机是为了增加特征之间的交互力度，使用多层cross layer对输入向量进行特征交叉。单层cross layer的基本操作是将cross layer的输入向量xl与原始的输入向量x0进行交叉，并加入bias向量和原始xl输入向量。DCN本质上还是对Wide\&Deep Wide部分表达能力不足的问题进行改进，与DeepFM的思路非常类似。

一个DCN模型从嵌入和堆积层开始，接着是一个交叉网络和一个与之平行的深度网络，之后是最后的组合层，它结合了两个网络的输出\cite{Recommender_System_With_Deep_Learning_DCN}。

论文：[DCN] Deep \& Cross Network for Ad Click Predictions (Stanford 2017)

\subsection{嵌入和堆叠层}
我们考虑具有离散和连续特征的输入数据。在网络规模推荐系统中，如CTR预测，输入主要是分类特征，如“country=usa”。这些特征通常是编码为独热向量如“[ 0,1,0 ]”；然而，这往往导致过度的高维特征空间大的词汇。

为了减少维数，我们采用嵌入过程将这些离散特征转换成实数值的稠密向量（通常称为嵌入向量）：
$$
x_{\text{embed},i} = W_{\text{embed},i}x_i
$$

然后，我们将嵌入向量与连续特征向量叠加起来形成一个向量：
$$
x_0 = [x_{\text{embed},1}^T, \cdots, x_{\text{embed},k}^T, x_{\text{dense}}^T]
$$

拼接起来的向量$X_0$将作为我们Cross Network和Deep Network的输入。

\subsection{Cross Network}
交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：
$$
x_{l+1} = x_0x_l^Tw_l + b_l + x_l = f(x_l, w_l, b_l) + x_l
$$

一个交叉层的可视化如图所示:
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/DCN_Cross_Detail.png}
\end{figure}

可以看到，交叉网络的特殊结构使交叉特征的程度随着层深度的增加而增大。多项式的最高程度（就输入$X_0$而言）为$L$层交叉网络$L + 1$。如果用$L_c$表示交叉层数，$d$表示输入维度。然后，参数的数量参与跨网络参数为：$d * L_c * 2$ ($w$和$b$)

交叉网络的少数参数限制了模型容量。为了捕捉高度非线性的相互作用，模型并行地引入了一个深度网络。

\subsection{Deep Network}
深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：
$$
h_{l+1} = f(W_lh_l + b_l)
$$

\subsection{Combination Layer}
链接层将两个并行网络的输出连接起来，经过一层全链接层得到输出：
$$
p = \sigma\Big( \Big[ x_{L_1}^T, h_{L_2}^T\Big] W_{\text{logits}}\Big)
$$

如果采用的是对数损失函数，那么损失函数形式如下：
$$
\text{loss} = -\frac{1}{N} \sum_{i=1}^Ny_i\log(p_i) + (1-y_i)\log(1-p_i) + \lambda\sum_l||w_l||^2
$$


相对于DeepFM和DCN对于Wide\&Deep Wide部分的改进，NFM可以看作是对Deep部分的改进。NFM的全称是Neural Factorization Machines，\textbf{如果我们从深度学习网络架构的角度看待FM，FM也可以看作是由单层LR与二阶特征交叉组成的Wide\&Deep的架构，与经典W\&D的不同之处仅在于Deep部分变成了二阶隐向量相乘的形式}。再进一步，NFM从修改FM二阶部分的角度出发，用一个带Bi-interaction Pooling层的DNN替换了FM的特征交叉部分，形成了独特的Wide\&Deep架构。其中Bi-interaction Pooling可以看作是不同特征embedding的element-wise product的形式。这也是NFM相比Google Wide\&Deep的创新之处。

论文：[NFM] Neural Factorization Machines for Sparse Predictive Analytics (NUS 2017)

\section{注意力机制在推荐模型中的应用}
\subsection{AFM（2017年）——引入Attention机制的FM}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/AFM_Structure.jpg}
\end{figure}

AFM的全称是Attentional Factorization Machines，是串行结构中一种网络模型。AFM 模型可以被认为是之前介绍的 NFM 模型的延续。在 NFM 模型中，不同域的特征 Embedding 向量经过特征交叉池化层的交叉，将各交叉特征向量进行加和，输入最后由多层神经网络组成的输出层。问题的关键在于加和池化（Sum Pooling）操作，它相当于一视同仁地对待所有交叉特征，不考虑不同特征对结果的影响程度，事实上小结了大量有价值的信息。这里注意力机制就派上了用场，它基于假设——\textbf{不同的交叉特征对于结果的影响程度不同}。

通过前面的介绍我们很清楚的知道，FM其实就是经典的Wide\&Deep结构，其中Wide部分是FM的一阶部分，Deep部分是FM的二阶部分，而AFM顾名思义，就是引入Attention机制的FM。
所以，AFM其实是对FM的二阶部分的每个交叉特征赋予了权重，这个权重控制了交叉特征对最后结果的影响，也就非常类似于NLP领域的注意力机制（Attention Mechanism）。

具体到模型结构上，AFM 模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络（Attention Net）实现的。注意力网络的作用是为每一个交叉特征提供权重，也就是注意力得分。

AFM利用Attention Net训练好Attention权重后，再反向作用于FM二阶交叉特征之上，使FM获得根据样本特点调整特征权重的能力。

同 NFM 一样，AFM 的特征交叉过程也使用了元素积操作，如下：
$$
f_{\text{PI}}(\varepsilon) = \{(v_i \odot v_j) x_i x_j\}_{(i,j) \in R_x}
$$

AFM 加入注意力得分后的池化过程为：
$$
f_{\text{Att}}(f_{\text{PI}}(\varepsilon)) = \sum_{(i,j) \in R_x}a_{ij}(v_i \odot v_j) x_ix_j
$$

对注意力得分 $a_{ij}$ 来说，最简单的方法就是用一个权重参数来表示，但为了防止交叉特征数据稀疏问题带来的权重参数难以收敛，AFM 模型使用了一个在两两特征交叉层（Pair-wise Interaction Layer）和池化层之间的注意力网络来生成注意力得分。

该注意力网络的结构是一个简单的单全连接层加 softmax 输出层的结构，如下：
$$
a'_{ij} = h^T \text{ReLU} (W(v_i \odot v_j)x_ix_j + b)
$$
$$
a_{ij} = \frac{\exp(a'_{ij})}{\sum_{(i,j) \in R_x}\exp(a'_{ij})}
$$

其中要学习的模型参数就是特征交叉层到注意力网络全连接层
的权重矩阵 $W$，偏置向量 $b$，以及全连接层到 softmax 输出层的权重向量 $h$。注意力网络将与整个模型一起参与梯度反向传播的学习过程，得到最终的权重参数。

论文：[AFM] Attentional Factorization Machines - Learning the Weight of Feature Interactions via Attention Networks (ZJU 2017)

\subsection{阿里DIN（2018年）——阿里加入Attention机制的深度学习网络}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Ali_DIN_Structure.jpg}
\end{figure}

AFM在FM中加入了Attention机制，2018年，阿里巴巴正式提出了融合了Attention机制的深度学习模型——Deep Interest Network。与AFM将Attention与FM结合不同的是，DIN将Attention机制作用于深度神经网络，在模型的embedding layer和concatenate layer之间加入了attention unit，使模型能够根据候选商品的不同，调整不同特征的权重。

论文：[DIN] Deep Interest Network for Click-Through Rate Prediction (Alibaba 2018)

后面有 DIN 模型的详细介绍。

\subsection{注意力机制对推荐系统的启发}
\textbf{注意力机制在数学形式上只是将过去的平均操作或加和操作换成了加权平均操作或者加权和操作}。这一机制对深度学习推荐系统的启发是重大的，因为注意力得分的引入反映了人类天生的注意力机制特点。对这一机制的模拟，使得推荐系统更加接近用户真实的思考过程，从而达到提升推荐系统效果的目的。

\section{DIEN——序列模型与推荐系统的结合}
\subsection{DIEN 简介}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Ali_DIEN_Structure.jpg}
\end{figure}

DIEN的全称为Deep Interest Evolution Network，它不仅是对DIN的进一步“进化”，更重要的是\textbf{DIEN通过引入序列模型 AUGRU模拟了用户兴趣进化的过程}。具体来讲模型的主要特点是在Embedding layer和Concatenate layer之间加入了生成兴趣的Interest Extractor Layer和模拟兴趣演化的Interest Evolving layer。其中Interest Extractor Layer使用了DIN的结构抽取了每一个时间片内用户的兴趣，Interest Evolving layer则利用序列模型AUGRU的结构将不同时间的用户兴趣串联起来，形成兴趣进化的链条。最终再把当前时刻的“兴趣向量”输入上层的多层全连接网络，与其他特征一起进行最终的CTR预估。

论文：[DIEN] Deep Interest Evolution Network for Click-Through Rate Prediction (Alibaba 2019)

\subsection{提出背景\cite{Recommender_System_With_Deep_Learning_DIEN}}
在大多数非搜索电商场景下，用户并不会实时表达目前的兴趣偏好。因此通过设计模型来捕获用户的动态变化的兴趣，是提升CTR预估效果的关键。阿里之前的DIN模型将用户的历史行为来表示用户的兴趣，并强调了用户兴趣的多样性和动态变化性，因此通过attention-based model来捕获和目标物品相关的兴趣。虽然DIN模型将用户的历史行为来表示兴趣，但存在两个缺点：

1）用户的兴趣是不断进化的，而DIN抽取的用户兴趣之间是独立无关联的，没有捕获到兴趣的动态进化性；

2）通过用户的显式的行为来表达用户隐含的兴趣，这一准确性无法得到保证。

基于以上两点，阿里提出了深度兴趣演化网络DIEN来CTR预估的性能。DIEN模型的主要贡献点在于：

1）模型\textbf{关注电商系统中兴趣演化的过程}，并提出了新的网络结果来建模兴趣进化的过程，这个模型能够更精确的表达用户兴趣，同时带来更高的CTR预估准确率。

2）设计了\textbf{兴趣抽取层}，并通过计算一个辅助loss，来提升兴趣表达的准确性。

3）设计了\textbf{兴趣进化层}，来更加准确的表达用户兴趣的动态变化性。

\subsection{DIEN模型原理}
\subsubsection{模型总体结构}
我们先来对比一下DIN和DIEN的结构。
DIN的模型结构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DIEN_Compare_DIN.png}
\end{figure}

DIEN的模型结构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DIEN_Compare_DIEN.png}
\end{figure}

可以看到，DIN和DIEN的最底层都是Embedding Layer，User profile， target AD和context feature的处理方式是一致的。不同的是，DIEN将user behavior组织成了序列数据的形式，并把简单的使用外积完成的activation unit变成了一个attention-based GRU网络。

\subsubsection{兴趣抽取层Interest Extractor Layer}
兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用\textbf{GRU单元}来提取interest。GRU单元的表达式如下：
\begin{align*}
u_t &= \sigma(W^ui_t + U^uh_{t-1} + b^u) \\
r_t &= \sigma(W^ri_t + U^rh_{t-1} + b^r) \\
\tilde{h}_t &= \tanh(W^hi_t + r^t \circ U^h h_{t-1} + b^h) \\
h_t &= (1-u_t) \circ h_{t-1} + u_t \circ \tilde{h}_t
\end{align*}

这里我们可以认为$h_t$是提取出的用户兴趣，但是这个地方兴趣是否表示的合理呢？文中别出心裁的增加了一个辅助loss，来提升兴趣表达的准确性：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DIEN_Compare_DIEN.png}
\end{figure}

这里，作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为$e(t+1)$作为正例，负采样得到的行为作为负例$e(t+1)'$，分别与抽取出的兴趣$h(t)$结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失：
$$
L_{aux}  = -\frac{1}{N}(\sum_{i=1}^N\sum_t\log\sigma(h_t, e_b^i[t+1]) + \log(1-\sigma(h_t, \hat{e}_b^i[t+1])))
$$

\subsubsection{兴趣进化层Interest Evolution Layer}
兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。举个简单的例子：

以用户对衣服的interest为例，随着季节和时尚风潮的不断变化，用户的interest也会不断变化。这种变化会直接影响用户的点击决策。建模用户兴趣的进化过程有两方面的好处：
1）追踪用户的interest可以使我们学习final interest的表达时包含更多的历史信息。
2）可以根据interest的变化趋势更好地进行CTR预测。

而interest在变化过程中遵循如下规律：
1）\textbf{interest drift}：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。
2）\textbf{interest individual}：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。

为了利用这两个时序特征，我们需要再增加一层GRU的变种，并加上attention机制以找到与target AD相关的interest。

attention的计算方式如下：
$$
a_t = \frac{\exp{h_tWe_a}}{\sum_{j=1}^T\exp{h_jWe_a}}
$$

而Attention和GRU结合起来的机制有很多，文中介绍了一下三种：

\textbf{GRU with attentional input (AIGRU)}

这种方式将attention直接作用于输入，无需修改GRU的结构：
$$
i'_t = h_t * a_t
$$

\textbf{Attention based GRU(AGRU)}

这种方式需要修改GRU的结构，此时hidden state的输出变为：
$$
h'_t = (1-a_t)*h'_{t-1} + a_t * \bar{h}'_t
$$

\textbf{GRU with attentional update gate (AUGRU)}

这种方式需要修改GRU的结构，此时hidden state的输出变为:
$$
\bar{u}'_t = a_t * u'_t
$$
$$
h'_t = (1-\bar{u}'_t) \circ h'_{t-1} + \bar{u}'_t \circ \bar{h}'_t
$$

\subsection{模型效果}
DIEN使用了辅助loss和AUGRU结构，而BaseModel + GRU + AUGRU与DIEN的不同之处就是没有增加辅助loss。可以看到，DIEN的实验效果远好于其他模型。

\section{强化学习与推荐系统的结合}
\subsection{DRN 模型\cite{Recommender_System_With_Deep_Learning_DRN}}
论文：DRN: A Deep Reinforcement Learning Framework for News Recommendation

\subsubsection{背景}
新闻领域的个性化推荐十分重要，传统的方法如基于内容的方法、协同过滤、深度学习方法在建模user-item交互关系时，经常面临以下三个问题：

1）难以处理新闻推荐的动态变化。这种动态变化体现在两个方面，首先新闻具有很强的时效性，其次是用户对于新闻阅读的兴趣是不断变化的。因此，在建模过程中，不仅要考虑用户对当前推荐的反馈，还要考虑长期的影响。就好比买股票，不能只考虑眼前的收益，而是要考虑未来的预期收益。

2）当前的推荐算法通常只考虑用户的点击／未点击 或者 用户的评分作为反馈，然而，用户隔多久会再次使用服务也能在一定程度上反映用户对推荐结果的满意度。

3）目前的推荐系统倾向于推荐用户重复或相似内容的东西，这也许会降低用户在同一个主题上的兴趣度。因此需要进行exploration。传统方法 e-greedy strategy 或者 Upper Con dence Bound (UCB) 都会在短期对推荐系统的效果造成一定的影响，需要更有效的exploration策略。

因此，本文提出了基于强化学习的推荐系统框架来解决上述提到的三个问题：
1）首先，使用\textbf{DQN网络}来有效建模新闻推荐的动态变化属性，DQN可以将短期回报和长期回报进行有效的模拟。

2）将\textbf{用户活跃度（activeness score）}作为一种新的反馈信息，用户活跃度在后面会详细介绍。

3）使用\textbf{Dueling Bandit Gradient Descent}方法来进行有效的探索。

\subsubsection{问题定义}
下面是本文中的一些符号约定：
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth]{fig/DRN_Notations.png}
\end{figure}

\subsubsection{算法框架}
算法的框架如下图所示：
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/DRN_Structure.png}
\end{figure}

\subsubsection{模型整体框架}
模型整体框架如下图所示：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_Structure_Detail.png}
\end{figure}

有几个关键的环节：

\textbf{PUSH}：在每一个时刻，用户发送请求时，agent根据当前的state产生k篇新闻推荐给用户，这个推荐结果是exploitation和exploration的结合。

\textbf{FEEDBACK}：通过用户对推荐新闻的点击行为得到反馈结果。

\textbf{MINOR UPDATE}：在每个时间点过后，根据用户的信息（state）和推荐的新闻（action）及得到的反馈（reward），agent会评估exploitation network Q 和 exploration network Q ̃ 的表现，如果exploitation network Q效果更好，则模型保持不动，如果 exploration network Q ̃ 的表现更好，exploitation network Q的参数将会向exploration network Q ̃变化。

\textbf{MAJOR UPDATE}：在一段时间过后，根据DQN的经验池中存放的历史经验，对exploitation network Q 模型参数进行更新。

\subsubsection{特征设计}
DQN每次的输入有下面四部分的特征：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item 新闻的特征：包括题目，作者，排名，类别等等，共417维；
    \item 用户的特征：包括用户在1小时，6小时，24小时，1周，1年内点击过的新闻的特征表示，共413*5=2065维；
    \item 新闻和用户的交互特征：25维；
    \item 上下文特征：32维的上下文信息，如时间，周几，新闻的新鲜程度等；
\end{itemize}

在这四组特征中，\textbf{用户特征和上下文特征用于表示当前的state，新闻特征和交互特征用语表示当前的一个action}。

\subsubsection{深度强化学习作推荐}
这里深度强化学习用的是Dueling-Double-DQN。之前我们介绍过DQN的三大改进，包括Double-DQN，Dueling-DQN和优先经验回放，这里用到了两个。将用户特征和上下文特征用于表示当前的state，新闻特征和交互特征用语表示当前的一个action，经过模型可以输出当前状态state采取这个action的预测Q值。

Q现实值包含两个部分：立即获得的奖励和未来获得奖励的折现：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_Q_Value.png}
\end{figure}

立即的奖励可能包含两部分，即用户的点击奖励和用户活跃度奖励。由于采取了Double-DQN 的结构，Q现实值的计算变为：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_Q_Value_Immediate.png}
\end{figure}

再加上Dueling的考虑，模型的网络结构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_Q_Value_Whole.png}
\end{figure}

\subsubsection{用户活跃度}
用户活跃度（User Activeness） 是本文提出的新的可以用作推荐结果反馈的指标。用户活跃度可以理解为使用app的频率，好的推荐结果可以增加用户使用该app的频率，因此可以作为一个反馈指标。用户活跃度的图示如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_User_Life.png}
\end{figure}


如果用户在一定时间内没有点击行为，活跃度会下降，但一旦有了点击行为，活跃度会上升。

在考虑了点击和活跃度之后，之前提到过的立即奖励变为：
$$
r_{total} = r_{click} + \beta r_{active}
$$

\subsubsection{探索}
本文的探索采取的是Dueling Bandit Gradient Descent 算法，算法的结构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DRN_Gradient_DBGD.png}
\end{figure}

在DQN网络的基础上又多出来一个exploration network Q ̃ ，这个网络的参数是由当前的Q网络参数基础上加入一定的噪声产生的，具体来说：
$$
\Delta W = \alpha \cdot rand(-1, 1) \cdot W
$$

当一个用户请求到来时，由两个网络同时产生top-K的新闻列表，然后将二者产生的新闻进行一定程度的混合，然后得到用户的反馈。如果exploration network Q ̃的效果好的话，那么当前Q网络的参数向着exploration network Q ̃的参数方向进行更新，具体公式如下：
$$
W' = W + \eta \tilde{W}
$$


\subsection{使用强化学习优化用户的长期体验——京东\cite{Recommender_System_With_Deep_Learning_Reinforcement_Learning}}
论文来源：Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems

\subsubsection{问题定义}
本文是京东和清华合作发表的论文，所以我们可以把它视作京东的推荐场景，用户会不断的与推荐系统推荐出的物品进行交互。当页面出现一个物品时，用户可以点击并查看消息详情、进行下单、忽略该物品、继续下滑查看更多物品或者关闭app。推荐系统要做的就是在这个过程中优化用户的体验。这种体验包括两方面，即时体验如用户点击率 和 长期体验如用户的黏度。用户的黏度又可以体现在许多方面，如用户在APP中的停留时间、打开APP的频率等等。

信息流推荐中，可以看作是第1次推荐-用户反馈-第2次推荐-用户反馈----第n次推荐-用户反馈这样的一个过程，或者说，我们可以将其定义为一个马尔可夫决策过程。因而可以通过强化学习方法来进行学习。对于一个马尔可夫决策过程，通常用<S、A、P、R、γ>来表示。在本文的推荐场景中，这几部分的定义分别如下：

\textbf{状态(State)S} : 在这里，我们假定每次推荐一个物品$i$。初始的状态$s_1=\{u\}$，即只有用户的信息。当进行了第一次推荐后，状态变为$s_2={u,(i_1,f_1,d_1)}$。当推荐过$t-1$个物品后，状态$s_t = {u,(i_1,f_1,d_1),(i_2,f_2,d_2),...,(i_{t-1},f_{t-1},d_{t-1})}$。即$s_t = s_{t-1} + \{(i_{t-1},f_{t-1},d_{t-1}\}$。这里$i_{t-1}$代表第${t-1}$时刻推荐的物品，$f_{t-1}$表示用户对物品$i_{t-1}$作出的反馈，$d_{t-1}$表示用户对推荐的物品$i_{t-1}$的停留时间。

\textbf{动作(Action)A}：这里的动作空间是可推荐的物品的集合，与当前的状态$s$相关，计作$A(s)$。初始阶段，所有的物品都可以进行推荐，随着时间步的推进，每推荐一个物品，就将其从可推荐物品集合中剔除。而时刻$t$的动作就是该轮推荐的物品$i_t$

\textbf{状态转移概率P}：状态转移概率可以用如下的式子表示：$p(s_{t+1} | s_t,i_t)$。

\textbf{奖励(Reward)R}: 由于我们不仅要优化用户的即时体验，还要优化用户的长期体验，因此这的即时奖励$r$比较复杂，这里将其定义为:
$$
r_t = \omega^T m_t
$$

可以理解为总的即时奖励$r$，是一堆分解奖励的加权平均，那具体包含哪些因素呢？这里主要包含以下三个方面：\textbf{点击次数}：用户对该次推荐的物品的点击次数；\textbf{滑动深度}：表示用户在经过该次推荐之后，又往下滑动了多少屏。也就是说，该次推荐之后，又经过了多少轮推荐。上面解释的可能比较晦涩，举个简单的例子。假设在一次信息流推荐的过程中，共进行了10轮，当前是第4轮，后面还有6轮，那么其滑动深度的奖励就是6 。\textbf{回归时间}：用户下次访问APP的时间，如果这个时间间隔越小，奖励就越大，所以通常表示为此次访问和下次访问APP中间的时间间隔的倒数：$m_t^r = \beta / v^r$

\subsection{模型介绍}
略

\subsection{S网络}
\textcolor{red}{为了离线训练Q网络，这里文中通过一个S网络来对真实环境进行模拟，用来计算即时奖励$r$。}

\section{总结—— CTR模型的深度学习时代}
结合自己的工作经验，关于深度学习模型我想再分享两点内容：

1. \textbf{没有银弹}。从来没有一个深度学习模型能够在所有数据集上都表现最优，特别是推荐、广告领域，各家的数据集，数据pattern、业务领域差异巨大，不存在能够解决一切问题的“银弹”模型。比如，阿里的DIEN对于数据质量、用户整个life cycle行为完整性的要求很高，如果在某些DSP场景下运用这个模型，预计不会收到良好的效果。再比如Google 的Deep\&Cross，我们也要考虑自己的数据集需不需要如此复杂的特征交叉方式，在一些百万量级的数据集上，也许浅层神经网络的表现更好。

2. \textbf{算法工程师永远要在理想和现实间做trade off}。有一种思想要避免，就是我为了要上某个模型就要强转团队的技术栈，强买某些硬件设备。模型的更新过程一定是迭代的，一定是从简单到复杂的，一定是你证明了某个模型是work的，然后在此基础上做改进的。这也是我们要熟悉所有模型演化关系的原因。


\part{推荐系统的工业化应用}
\section{推荐系统的「 实时性」\cite{Real_Time_Requirement_Of_Recommender_System}\cite{Real_Time_Requirement_Of_Model_Update}}
推荐系统的实时性是至关重要的，主要体现在下面两个方面：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textbf{推荐系统的更新速度越快，越能够反应用户最近的用户习惯，越能够给用户进行越有时效性的推荐。}
    \item \textbf{推荐系统更新的越快，模型更容易发现最新流行的数据pattern，越能够让模型反应找到最新的流行趋势。}
\end{itemize}

这两个方面的原因也直接与影响推荐系统实时性的两大要素有关：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textbf{推荐系统「 特征」的实时性；}
    \item \textbf{推荐系统「 模型」的实时性。}
\end{itemize}

\subsection{推荐系统“特征”的实时性}
\subsubsection{特征实时性的重要性}
推荐系统特征的实时性指的是系统“实时”地收集推荐系统模型所需的输入特征，使推荐系统能够总是使用最新的特征进行预测和推荐。

\begin{framed}
举例来说，现在开发一个短视频推荐系统，某用户完整地看完了一个长度为10分钟的“羽毛球教学”视频上。那么毫无疑问该用户对于“羽毛球”这个主题是感兴趣的。系统希望在用户下次翻页的时候就继续推荐“羽毛球”相关的视频。但是由于系统特征的实时性不强，用户的观看历史无法实时反馈给推荐系统，导致推荐系统在得知该用户看过“羽毛球教学”这个视频的时候，已经半个小时之后了，此时用户已经离开该应用了。这就是一个推荐系统实时性差导致推荐失败的例子。
\end{framed}

诚然，用户在下次开启该应用的时候，推荐系统可以利用上次的用户行为历史推荐“羽毛球”相关的视频，但该推荐系统毫无疑问丧失了最可能增加用户粘度的、增加用户留存度的时机。

那么如何增强“特征”的实时性呢？这里我简略画了一张推荐系统的主流技术架构图（图2），来说明影响“特征”实时性的三个主要阶段 。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Recommender_System_Data_Stream_Architecture.png}
    \caption*{图2 推荐系统数据流的技术架构图}
\end{figure}

\subsubsection{客户端实时特征}
客户端是最接近用户的环节，在经典的推荐系统中，经常利用客户端收集时间、地点、推荐场景等上下文特征，然后让这些特征随http请求一起到达服务器端，参与模型预测。但是客户端对于实时性的重要性，经常被忽视的一点是客户端还是能够实时收集session内用户行为的地方。

拿新闻类app来说，用户在同一session中，三分钟之内分别点击并阅读了三篇文章。这三篇文章对于用户的推荐结果来说是至关重要的，因为它们代表了用户的即时兴趣。如果采用传统的流计算平台，甚至分布式批处理计算平台，由于系统延迟问题，大概率无法在3分钟之内就把session内部的行为历史存储到特征数据库（比如redis）中，这就使这位用户的推荐结果不会马上受到session内部行为的影响。

如果\textbf{客户端能够缓存session内部的行为，作为与上下文特征同样的实时特征传给推荐服务器}，那么推荐模型就能够实时得到session内部行为特征，进行实时的推荐。这就是利用客户端实时特征进行实时推荐的优势所在。

\subsubsection{流处理平台的准实时特征处理}
随着storm，spark streaming，特别是flink等一批非常优秀的流处理平台的日益成熟。利用流处理平台进行准实时的特征处理已经成为了当前推荐系统的标配。

所谓流处理平台，是将日志以流的形式进行mini batch处理的准实时计算平台。由于每次需要等待并处理一小批日志，流处理平台并非完全实时的平台，但优势是能够进行一些简单的统计类特征的计算，比如一个物品在该时间窗口内的曝光次数，点击次数，一个用户在该时间窗口内的点击话题分布等等。

流处理平台计算出的特征可以立马存入特征数据库供推荐系统模型使用，虽然无法实时的根据用户行为改变用户结果，但分钟级别的延迟基本可以保证用户的推荐结果准实时地受到之前行为的影响。

\subsubsection{分布式批处理平台的全量特征处理}
随着数据最终到达以HDFS为主的分布式存储系统。Spark等分布式计算平台终于能够进行全量特征的计算和抽取。在这个阶段着重进行的还有多个数据源的数据join和以及延迟信号的合并。

比如用户的曝光、点击、转化数据往往是在不同时间到达HDFS的，有些游戏类应用的转化数据的延迟甚至高达几个小时，因此也只有在这一阶段才能够进行全量特征以及相应label的抽取和合并。也只有在全量特征准备好之后，才能够进行更高阶的特征组合的工作。这往往是无法在客户端和流处理平台平台上进行的。

分布式批处理平台的计算结果的主要用处有两个：(1) 模型训练和离线评估；(2) 特征保存入特征数据库，供推荐模型inference使用。

当然，由于数据从产生到完全进入HDFS，再加上spark的计算延迟，这一过程的总延迟往往达到小时级别，已经无法进行所谓的“实时”推荐。更多的是对用户下次登陆时进行更好的推荐。

\subsubsection{再谈推荐系统特征实时性的重要性}
在构建推荐系统时，推荐系统的实时性往往是容易被我们忽视的因素。我们一味聚焦在一些离线指标上，希望在离线指标中发现改进模型结构的线索，殊不知线上特征实时性的改动就会产生原强于模型结构的影响。

但特征实时性再强，影响的范围也仅限于当前用户，要想快速抓住系统级别的全局的数据变化和新产生的数据pattern，就必须加强“模型”的实时性。

\begin{framed}
在使用抖音时，随着你划过不同的短视频，你的兴趣快速收敛，几乎是实时地被抖音的推荐引擎捕捉，这是如何做到的？在刷知乎的timeline时，随着你点击不同的答案，知乎的推荐也是几乎实时的改变着推荐列表，比如你点击了我的这篇文章，在你下次更新知乎timeline时，立马会有更多推荐系统相关文章出现，这是如何做到的？

知乎和抖音这种体验，在信息流里都是基本操作。实现这个大概需要几点：

1、用户行为反馈实时回传；

2、根据用户实时行为有特定的实时召回源，比如同上次点击/本session内所有点击平均相似度最接近的N个内容；

3、保证实时召回源的内容可以排出去（如果有价值的话），这里需要通过rank模型相关特征保障：如果有召回源相关特征（match类特征）基本上已经可以做到，也就是如果实时召回的内容真的有价值，那么这个召回源召回的内容在排序时权重自然高，体现在该召回源的match类特征上；

4、另外还可以试试跟abb那样，增加每个待排序内容同用户近期点击的内容、近期没点击内容的相似度特征，abb的实践看这两个特征也很强；

5、使用产品策略也可以做到实时推荐，比如用户当前产生了点击，互动行为，或者打开了一条push，那么我们可以根据这条物料的标签，发博者信息等，立即返回相关的推荐物料（标签相似，发博者相似），一般是两三条强插到下一次刷新中。 数据显示这种策略推荐的物料互动率，点击率是普通物料的两三倍以上。
\end{framed}

\subsection{推荐系统模型更新的「实时性」}
\subsubsection{推荐系统“模型”的实时性}
与“特征”的实时性相比，\textbf{推荐系统模型的实时性往往是从更全局的角度考虑问题}。特征的实时性力图用更准确的特征描述一个人，从而让推荐系统给出更符合这个人的推荐结果。而模型的实时性则是希望更快的抓住全局层面的新的数据模式，发现新的趋势和相关性。

\begin{framed}
举例来说，拿某电商网站双十一的大量促销活动来说。特征的实时性会根据用户最近的行为更快的发现用户可能感兴趣的商品，但绝对不会发现用户相似人群最新的偏好，商品之间最新的相关性信息，新活动的趋势信息等。
\end{framed}

要发现这类全局性的数据变化，就需要更快地更新模型。而影响模型的实时性最重要的因素就是模型的训练方式（如图1）。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Model_Time_Delay_And_Training_Method.png}
    \caption*{图1 模型的实时性与训练方式}
\end{figure}

\subsubsection{全量更新}
模型训练最常用的方式就是全量更新。模型会利用某时间段内的所有训练样本进行重新训练，再用训练好的新模型替代“过时”的模型。

但全量更新需要训练的样本量大，因此所需训练时间较长；而且全量更新往往在离线的大数据平台上进行，如spark+tensorflow，因此数据的延迟也较长，这都导致了全量更新是“实时性”最差的模型更新方式。

事实上，对于已经训练好的模型，可以仅对新加入的增量样本进行学习就可以了，这就是所谓的增量更新。

\subsubsection{增量更新（Incremental Learning）}
\textbf{增量更新仅将新加入的样本喂入模型进行增量学习}。从技术上来说，深度学习模型往往采用随机梯度下降（SGD）以及其变种进行学习，模型对增量样本的学习相当于在原有样本的基础上继续输入增量样本进行梯度下降。因此在深度学习模型的基础上，由全量更新改为增量更新的难度并不大。

但工程上的任何事情都是tradeoff，永远不存在完美的解决方案，增量更新也不例外。由于仅利用增量样本进行学习，因此模型在多个epoch之后也是收敛到新样本的最优点，而很难收敛到原所有样本+增量样本 的全局最优点。

因此在实际的推荐系统中，往往\textbf{采用增量更新与全局更新相结合的方式}，在进行几轮增量更新后，在业务量较小的时间窗口进行全局更新，纠正模型在增量更新过程后中积累的误差。在“实时性”和“全局最优”中间进行取舍和权衡。

\subsubsection{在线学习（online learning）}
“在线学习“是“增量更新”的进一步改进，“增量更新”是在获得一批新样本时进行增量更新，而在线学习是在每次获得一个新样本的时候就实时更新模型。在线学习在技术上也可以通过SGD类的方式实现。但如果使用general的SGD方法，在线学习会导致一个很严重的问题，就是模型的稀疏性很差，打开过多“碎片化”的不重要特征。

我们关注模型的“稀疏性”某种意义上也是工程上的考虑，比如在一个输入向量达到几百万维的模型中，如果模型的稀疏性好的话，可以在模型效果不受影响的前提下，仅让极小一部分维度的输入向量的对应权重非零，也就是说在模型上线的时候，模型的体积是很小的，这无疑有利于整个模型serving的过程。无论是存储模型所需的内存空间，以及线上inference的速度都会因为模型的稀疏性收益。

如果使用SGD的方式进行模型更新，相比batch的方式，容易产生大量小权重的特征，这就增大了模型部署和更新的难度。那么为了在在线学习过程中兼顾训练效果和模型稀疏性，有大量相关的研究，最著名的包括微软的RDA，google的FOBOS和最著名的FTRL等 。

\subsubsection{模型局部更新}
提高模型实时性的另外一个改进方向是进行模型的局部更新，大致的思路是\textbf{降低训练效率低的部分的更新频率，提高训练效率高的部分的更新频率}。这种方式比较有代表性的是facebook的GBDT+LR模型。

该模型利用GBDT进行自动化的特征工程，利用LR拟合优化目标。由于GBDT是串行的依次训练每一颗树，因此训练效率不高，更新的周期较长，如果每次都同时训练GBDT+LR整个模型，那么GBDT的低效问题将拖慢LR的更新速度。因此为了兼顾GBDT的特征处理能力和LR快速拟合优化目标的能力，Facebook采取的部署方法时每天训练一次GBDT模型，固定GBDT模型后，准实时的训练LR模型以快速捕捉数据整体的变化。那么通过模型的局部更新，做到了GBDT和LR能力的权衡。

模型局部更新的做法也较多采用在Embedding层+神经网络的深度学习模型（例如图3的W\&D模型）中，因为Embedding层负责将高维稀疏输入向量转换为稠密embedding向量，因此Embedding层一层的参数往往占深度学习模型90\%以上。因此Embedding层的更新会拖累模型整体的更新速度，因此业界也往往采用embedding层单独训练甚至预训练，embedding层以上的模型部分高频更新的策略。

\subsubsection{客户端模型实时更新}
在前面章节中介绍“特征”实时性的部分，提到了客户端“实时特征”的方法。既然客户端是最接近用户的部分，实时性最强，那么能否在客户端就根据当前用户的行为历史更新模型呢？

客户端模型实时更新在业界也是仍处于探索阶段的方法。难点在于分布在每个客户端模型的更新与服务器端模型的更新的协同问题。也许可以采用类似parameter server架构的模型更新策略，但本质上，\textbf{模型的更新不会对用户的推荐产生十分显著的影响，采用客户端模型实时更新的必要性也不是非常大}。这里想介绍的其实是一种简化的客户端模型更新策略，用户embedding的实时更新。

user embedding实时更新的逻辑和动机是，在深度学习推荐系统中，模型往往要接受用户embedding和物品embedding两个关键的特征向量。对于物品embedding的更新来说，往往需要全局的数据，因此只能在服务器端进行整体的更新；而对用户embedding来说，则更多依赖于用户自身的数据。那么把用户embedding的更新过程移植到客户端来做，就能够实时地把用户最近的行为数据反应到用户的embedding中来，从而通过实时改变用户embedding的方式完成实时推荐。

这里用一个最简单的例子来说明该过程。例如用户embedding由用户点击过的物品embedding进行平均得到，那么客户端是最先得到用户最新点击物品信息的，那么就可以实时更新用户embedding，在下次推荐时，将更新后的用户embedding传给服务器端，让服务器端根据最新的用户embedding返回实时推荐内容。

当然，用户embedding的生成过程可以使用更复杂的网络和embedding方法进行生成，如果能与整个网络解耦合，并单独部署到客户端，同样可以进行实时更新。

\section{Facebook的经典CTR预估模型}
论文：Practical Lessons from Predicting Clicks on Ads at Facebook

\subsection{用户场景}
论文中的用户场景是一个标准的点击率预估的场景，需要强调的只有一点，因为我们需要利用CTR计算精准的出价、ROI等重要的后续预估值，因此CTR模型的预估值需要是一个\textbf{具有物理意义的精准的CTR，而不是仅仅输出广告排序的高低关系}。所以文中不仅把CTR calibration作为重要的评价指标，更是在最后介绍了模型校正的相关方法。

\subsection{模型结构}
文章提出了一种利用GBDT自动进行特征筛选和组合，进而生成新的feature vector，再把该feature vector当作logistic regression的模型输入，预测CTR的模型结构。
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_GBDT_LR_Feature.jpg}
\end{figure}

这里需要强调的是，\textbf{用GBDT构建特征工程，和利用LR预测CTR两步是独立训练的}。所以自然不存在如何将LR的梯度回传到GBDT这类复杂的问题，而利用LR预测CTR的过程是显然的，在此不再赘述，我们着重讲一讲如何利用GBDT构建新的特征向量。

大家知道，GBDT是由多棵回归树组成的树林，后一棵树利用前面树林的结果与真实结果的残差做为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构自然进行了有效的特征组合，也就非常高效的解决了过去非常棘手的特征选择和特征组合的问题。

我们利用训练集训练好GBDT模型，之后就可以利用该模型构建特征工程。具体过程是这样的，一个样本在输入GBDT的某一子树后，会根据每个节点的规则最终落入某一叶子节点，那么我们把该叶子节点置为1，其他叶子节点置为0，所有叶子节点组成的向量即形成了该棵树的特征向量，把GBDT所有子树的特征向量concatenate起来，即形成了后续LR输入的特征向量。

举例来说，比如GBDT由三颗子树构成，每个子树有4个叶子节点，一个训练样本进来后，先后落到了“子树1”的第3个叶节点中，那么特征向量就是[0,0,1,0]，“子树2”的第1个叶节点，特征向量为[1,0,0,0]，“子树3”的第4个叶节点，特征向量为[0,0,0,1]，最后concatenate所有特征向量，形成的最终的特征向量为[0,0,1,0,1,0,0,0,0,0,0,1]，我们再把该向量作为LR的输入，预测CTR。

引入了GBDT+LR的模型后，相比单纯的LR和GBDT，提升效果是非常显著的。从论文中可以看到，混合模型比单纯的LR或Trees模型在loss上减少了3%。

该模型的优势我们上面已经提到，即可以自动进行特征组合和特征筛选，但在实践过程中，模型的缺陷也比较明显，相比FTRL，FM，NN等能够通过梯度下降训练的模型来说，\textbf{GBDT缺乏online learning的能力，因此我们往往只能相隔一天甚至几天才能够update GBDT模型，势必影响模型的实效性}，那么Facebook是如何解决模型更新的问题的呢？

\subsection{模型的实效性问题和更新策略}
虽然我们的直觉是模型的训练时间和serving时间之间的间隔越短，模型的效果越好，但为了证明这一点，facebook的工程师还是做了一组实效性的实验，在结束模型的训练之后，观察了其后6天的模型loss（这里采用normalized entropy作为loss）
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_GTDT_LR_Training_Delay.jpg}
    \caption{模型更新延迟与loss的关系}
\end{figure}

可以看出，模型的loss在第0天之后有所上升，特别是第2天过后显著上升。因此daily update的模型相比weekly update的模型效果肯定是有大幅提升的。

但囿于facebook巨大的数据量以及GBDT较难实施并行化的原因，GBDT的更新时间往往超过24小时，所以为了兼顾data freshness和客观的工程要求，facebook采取了下面的模型更新方法：

The boosted decision trees can be trained daily or every couple of days, but the linear classifier can be trained in near real-time by using some flavor of online learning.

就是说GBDT的部分几天更新一次，而LR的部分进行准实时的更新，这无疑是很好的工程实践经验。时至今日，我们已经开始使用大量不同的embedding方法进行特征编码，facebook当时的做法也对我们现在的工程实践有重要的参考价值。因为大量深度学习embedding方法的更新计算开销也非常大，但对实效性要求并不高，我们也完全可以低频更新embedding，高频或实时更新基于embedding特征的LR，NN等预测模型。

\subsection{facebook的实时数据流架构}

为了实现模型的准实时训练，facebook专门介绍了其基于Scribe的数据流架构，文中称其为online data joiner。
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_Online_Data_Joiner.jpg}
\end{figure}

该模块最重要的作用是准实时的把来自不同数据流的数据整合起来形成sample features，并最终与click数据进行join，形成完整的labeled sample。在整个过程中，我认为最应该注意的有三点：

(1). \textbf{waiting window的设定}：waiting window指的是在impression发生后，我们要等待多久才能够判定一个impression是否有click。如果waiting window过大，数据实时性受影响，如果waiting window过小，会有一部分click来不及join到impression，导致样本CTR与真实值不符。这是一个工程调优的问题，需要有针对性的找到跟实际业务匹配的合适的waiting window。除此之外，无论怎样我们都会漏掉一部分click，这就要求我们阶段性的全量retrain我们的模型，避免online learning误差的积累。

(2). \textbf{分布式的架构与全局统一的action id}：为了实现分布式架构下impression记录和click记录的join，facebook除了为每个action建立全局统一的request id外，还建立了HashQueue缓存impressions。hashQueue缓存的impression，如果在窗口过期时还没有匹配到click就会当作negative sample，这里说的窗口与上面提到的waiting window相匹配。facebook使用scribe实现了这一过程，更多公司使用Kafka完成大数据缓存和流处理。

(3). \textbf{数据流保护机制}：facebook专门提到了online data joiner的保护机制，因为\textcolor{red}{一旦data joiner失效，比如click stream无法join impression stream，那么所有的样本都会成为负样本，由于模型实时进行online learning和serving，模型准确度将立刻受到错误样本数据的影响，进而直接影响广告投放和公司利润，后果是非常严重的}。为此，facebook专门设立了异常检测机制，一旦发现实时样本数据流的分布发生变化，将立即切断online learning的过程，防止预测模型受到影响。

\subsection{降采样和模型校正}
对于巨型互联网公司来说，为了控制数据规模，降低训练开销，降采样几乎是通用的手段，facebook实践了两种降采样的方法，uniform subsampling和 negative down sampling

uniform subsampling是对所有样本进行无差别的随机抽样，为选取最优的采样频率，facebook试验了0.001，0.01， 0.1， 0.5 和1五个采样频率，loss的比较如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_DownSample.jpg}
\end{figure}
可以看到当采样率是10\%时，相比全量数据训练的模型，仅损失了不到1\%的效果。

另一种方法 negative down sampling 保留全量正样本，对负样本进行降采样。除了提高训练效率外，负采样还直接解决了正负样本不均衡的问题，facebook经验性的选择了从0.0001到0.1的一组负采样频率，试验效果如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_Negative_Down_Sampling.jpg}
\end{figure}
大家可以看到，当负采样频率在0.025时，loss不仅优于更低的采样频率训练出来的模型，居然也优于负采样频率在0.1时训练出的模型，虽然原文没有作出进一步的解释，但推测最可能的原因是解决了数据不均衡问题带来的效果提升。

\textbf{负采样带来的问题是CTR预估值的漂移}，比如真实CTR是0.1\%，进行0.01的负采样之后，CTR将会攀升到10\%左右。而为了进行准确的竞价以及ROI预估等，CTR预估模型是要提供准确的有物理意义的CTR值的，因此在进行负采样后需要进行CTR的校正，使CTR模型的预估值的期望回到0.1\%。校正的公式如下：
$$
q = \frac{p}{p + (1-p)/w}
$$

其中q是校正后的CTR，p是模型的预估CTR，w是负采样频率。大家可以利用简单的转换关系就可以得出上述公式，有兴趣的同学可以手动推导一下。

\section{Facebook的DLRM模型}
论文：Deep Learning Recommendation Model for Personalization and Recommendation Systems

\subsection{模型架构}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_DLRM_Structure.jpg}
\end{figure}

从下到上来解释一下Facebook DLRM这个模型的模型结构：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textbf{特征工程}：所有特征被分为两类，一类是将类别、id类特征用one hot编码生成的稀疏特征（sparse features），一类是数值型连续特征（dense features）
    \item \textbf{Embedding层}：每个类别类特征转换成one hot vector后，用embedding层转换成维度为 $n$ 的embedding。也就是说，每种稀疏特征转换成一个embedding向量。而年龄、收入等连续型特征将被concat成一个特征向量后，输入图中黄色的MLP（Multi Layer Perceptron）中，被转化成同样维度为 $n$ 的向量。至此，无论是类别类稀疏特征，还是连续型特征组成的特征向量在经过Embedding层后，都被转换成了 $n$ 维的embedding向量。
    \item \textbf{NNs层}：Embedding再往上是由三角形代表的MLP神经网络层，也就是说得到n维的embedding 向量后，每类embedding还有可能进一步通过MLP做转换，原文中这么说的“ optionally passing them through MLPs”，就是说选择性的通过MLP做进一步转换，看来那三个三角形其实是根据调参情况可有可无的。
    \item \textbf{interactions层}：这一层其实这篇文章相对还算创新的一点，它是怎么做的呢？原文这么说的：This is done by taking the dot product between all pairs of embedding vectors and processed dense features. These dot products are concatenated with the original processed dense features and post-processed with another MLP (the top or output MLP)。也就是说会将之前的 embedding 两两做点积，做完之后再跟之前dense features对应的embedding concat起来，喂给后续的MLP。所以这一步其实是希望特征之间做充分的交叉，组合之后，再进入上层MLP做最终的目标拟合。这一点其实follow了FM的特征交叉概念。
    \item 最上层那个三角不用多说，是另一个FC NN，并最终用sigmoid函数给出最终的点击率预估。
\end{itemize}

整个模型看下来，没有特别fancy的结构，也没有加入sequential model，RF等模型的思路，可以说是一个工业界的标准深度学习模型。facebook的博客上还给出了稍微详细一点的模型结构图，大家可以参考。
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/Facebook_DLRM_Structure_Detail.jpg}
\end{figure}

\subsection{Facebook的模型并行训练方法}
作为一篇业界的论文，模型的实际训练方法，训练平台往往是让人受益最多的。而对于Facebook的数据量来说，单节点的模型训练必然无法快速完成任务，那么模型的并行训练就肯定是少不了的解决方法。我们先看原文是如何解释DLRM这个模型的并行训练过程的：

Our parallelized DLRM will use a combination of model parallelism for the embeddings and data parallelism for the MLPs to mitigate the memory bottleneck produced by the embeddings while parallelizing the forward and backward propagations over the MLPs.

简单来说，DLRM融合使用了模型并行和数据并行的方法，对于Embedding部分采用了模型并行，对于MLP部分采用了数据并行。Embedding部分采用模型并行的原因是减轻大量Embedding参数带来的内存瓶颈问题。MLP部分采用数据并行是可以并行进行前向和反向传播。

其实原文中并没有给出非常准确的并行训练方法，这里凭我自己的理解进一步解释一下Embedding做模型并行训练和上层MLP做数据并行训练的原理，对pytorch和caffe2熟悉的专家可以随时纠正我的解释：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item Embedding做模型并行训练指的是在一个device或者说计算节点上，仅有一部分Embedding层参数，每个device进行并行mini batch梯度更新时，仅更新自己节点上的部分Embedding层参数。
    \item MLP层和interactions进行数据并行训练指的是每个device上已经有了全部模型参数，每个device上利用部分数据计算gradient，再利用allreduce的方法汇总所有梯度进行参数更新。
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Facebook_Parallel_Training.png}
    \caption{模型并行训练示意图}
\end{figure}

\subsection{DLRM模型的效果}
在性能的对比上，DLRM选择了Google 2017年的DCN（Deep cross network）作为baseline。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Facebook_Google_DCN_Structure.jpg}
    \caption{Google DCN模型结构}
\end{figure}

DCN可以看作是对Wide\&Deep模型的进一步改进，主要的思路使用Cross网络替代了原来的Wide部分。其中设计Cross网络的基本动机是为了增加特征之间的交互力度，使用多层cross layer对输入向量进行特征交叉。单层cross layer的基本操作是将cross layer的输入向量xl与原始的输入向量x0进行交叉，并加入bias向量和原始xl输入向量。

可以看出DLRM和DCN的主要区别在于特征交叉方式的不同，DLRM采用了不同特征域两两点积的交叉方式，DCN采用了每个维度两两交叉的方式。利用Criteo Ad Kaggle data作为测试集，二者的性能对比如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Facebook_DLRM_DCN_Compare.jpg}
    \caption{DLRM与DCN性能对比}
\end{figure}

可以看到，DLRM在Accuracy上稍胜一筹。当然模型的performance与数据集的选择，参数的调优都有很大关系。而且DLRM在Adagrad训练方式下的优势已经微乎其微，这里的性能评估大家仅做参考即可。

\section{Youtube的深度学习推荐系统}
论文：Deep Neural Networks for YouTube Recommendations

\subsection{提出背景\cite{Recommender_System_With_Deep_Learning_Youtube}}
youtube是世界上最大的视频内容平台，在如此体量的平台中，推荐系统是至关重要的。但是，youtube的视频推荐面临三方面的挑战：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item Scale：视频和用户数量巨大，很多现有的推荐算法能够在小的数据集上表现得很好，但是在这里效果不佳。需要构建高度专业化的分布式学习算法和高效的服务系统来处理youtube庞大的用户和视频数量；
    \item Freshness：这体现在两方面，一方面视频更新频繁，另一方面用户行为更新频繁；
    \item Noise：相较于庞大的视频库，用户的行为是十分稀疏的，同时，我们基本上能获得的都是用户的隐式反馈信号。构造一个强健的系统是十分困难的。
\end{itemize}



\subsection{算法基本架构}
Youtube的算法架构如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{fig/YouTube_Algorithm_Architect.jpg}
\end{figure}

Youtube的用户推荐场景自不必多说，作为全球最大的UGC的视频网站，需要在百万量级的视频规模下进行个性化推荐。由于候选视频集合过大，考虑online系统延迟问题，不宜用复杂网络直接进行推荐，所以Youtube采取了两层深度网络完成整个推荐过程：

(1). 第一层是Candidate Generation Model完成候选视频的快速筛选，这一步候选视频集合由百万降低到了百的量级。

(2). 第二层是用Ranking Model完成几百个候选视频的精排

\subsection{召回层}
首先介绍candidate generation模型的架构：
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Youtube_Candidate_Generation_Model.jpg}
\end{figure}

我们自底而上看这个网络，最底层的输入是用户观看过的video的embedding向量，以及搜索词的embedding向量。至于这个embedding向量是怎么生成的，作者的原话是这样的

Inspired by continuous bag of words language models, we learn high dimensional embeddings for each video in a fixed vocabulary and feed these embeddings into a feedforward neural network

所以作者是先用word2vec方法对video和search token做了embedding之后再作为输入的，这也是做embedding的“基本操作”，不用过多介绍；当然，除此之外另一种大家应该也比较熟悉，就是通过加一个embedding层跟上面的DNN一起训练，两种方法孰优孰劣，有什么适用场合，大家可以讨论一下。

特征向量里面还包括了用户的地理位置的embedding，年龄，性别等。然后把所有这些特征concatenate起来，喂给上层的ReLU神经网络。

三层神经网络过后，我们看到了softmax函数。这里Youtube的同学们把这个问题看作为用户推荐next watch的问题，所以输出应该是一个在所有candidate video上的概率分布，自然是一个\textbf{多分类}问题。

\subsection{排序层}
\subsubsection{模型结构}
既然得到了几百个候选集合，下一步就是利用ranking模型进行精排序，下面是ranking深度学习网络的架构图。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{fig/Youtube_Ranking_Model.jpg}
\end{figure}

乍一看上面的ranking model似乎与candidate generation模型没有什么区别，模型架构还是深度学习的“基本操作”，唯一的区别就是特征工程，那么我们就讲讲特征工程。

\subsubsection{输入特征}
事实上原文也明确说明了，引入另一套DNN作为ranking model的目的就是引入更多描述视频、用户以及二者之间关系的特征，达到对候选视频集合准确排序的目的。

During ranking, we have access to many more features describing the video and the user's relationship to the video because only a few hundred videos are being scored rather than the millions scored in candidate generation.

具体一点，从左至右的特征依次是
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item impression video ID embedding: 当前要计算的video的embedding
    \item watched video IDs average embedding: 用户观看过的最后N个视频embedding的average pooling
    \item language embedding: 用户语言的embedding和当前视频语言的embedding
    \item time since last watch: 自上次观看同channel视频的时间
    \item \#previous impressions: 该视频已经被曝光给该用户的次数
\end{itemize}

上面五个特征中，我想重点谈谈第4个和第5个。因为这两个很好的引入了对用户行为的观察。

第4个特征背后的思想是：We observe that the most important signals are those that describe a user's previous interaction with the item itself and other similar items.

有一些引入attention的意思，这里是用了time since last watch这个特征来反应用户看同类视频的间隔时间。从用户的角度想一想，假如我们刚看过“DOTA经典回顾”这个channel的视频，我们很大概率是会继续看这个channel的视频的，那么该特征就很好的捕捉到了这一用户行为。

第5个特征\#previous impressions则一定程度上引入了exploration的思想，避免同一个视频持续对同一用户进行无效曝光。尽量增加用户没看过的新视频的曝光可能性。

\subsubsection{特征处理}
特征处理主要包含对于离散变量的处理和连续变量的处理。

对于离散变量，这里主要是视频ID，Youtube这里的做法是有两点：

(1). 只保留用户最常点击的N个视频的embedding，剩余的长尾视频的embedding被赋予全0值。可能的解释主要有两点，一是出现次数较少的视频的embedding没法被充分训练。二是也可以节省线上服务宝贵的内存资源。

(2). 对于相同域的特征可以共享embedding，比如用户点击过的视频ID，用户观看过的视频ID，用户收藏过的视频ID等等，这些公用一套embedding可以使其更充分的学习，同时减少模型的大小，加速模型的训练。


对于连续特征，主要进行归一化处理，神经网络对于输入的分布及特征的尺度是十分敏感。因此作者设计了一种积分函数将连续特征映射为一个服从[0,1]分布的变量。该积分函数为：
$$
\tilde{x} = \int_{-\infty}^{x}df
$$

个人理解就是将概率密度分布转换成了累计密度分布。为了引入特征的非线性，除了加入归一化后的特征外，还加入了该特征的平方和开方值。



\subsection{离线训练}
从模型结构可以看出，在离线训练阶段，我们将其视为了一个分类问题。我们使用隐式反馈来进行学习，用户完整观看过一个视频，便视作一个正例。如果将视频库中的每一个视频当作一个类别，那么在时刻$t$，对于用户$U$和上下文$C$，用户会观看视频i的概率为：
$$
P(w_t = i|U,C) = \frac{e^{v_iu}}{\sum_{j \in V}{e^{v_ju}}}
$$

其中，$u$是用户的embedding，这个embedding，是网络最后一个Relu激活函数的输出，$v_i$是视频$i$的embedding。那么问题来了，输入时，每一个视频也有一个对应的embedding，这个embedding是不是计算softmax的embedding呢？这里文章也没有说清楚。

使用多分类问题的一个弊端是，我们有百万级别的classes，模型是非常难以训练的，因此在实际中，Youtube使用负样本采样(negative sampling)的方法，将class的数量减小。还有一种可以替换的方法，成为hierarchical softmax，但经过尝试，这种方法并没有取得很好的效果。这里的负采样和word2vec中的负采样方法是不同的，这里采样之后还是一个多分类问题，而word2vec中的负采样方法是将问题转为了一个二分类问题。

\subsection{工程实践}
\subsubsection{softmax的训练}
文中把推荐问题转换成多分类问题，在预测next watch的场景下，每一个备选video都会是一个分类，因此总共的分类有数百万之巨，这在使用softmax训练时无疑是低效的，这个问题YouTube是如何解决的？

这个问题原文的回答是这样的：We rely on a technique to sample negative classes from the background distribution ("candidate sampling") and then correct for this sampling via importance weighting.

简单说就是进行了负采样（negative sampling）并用importance weighting的方法对采样进行calibration。文中同样介绍了一种替代方法，hierarchical softmax，但并没有取得更好的效果。当然关于采样的具体技术细节以及优劣可能再开一篇文章都讲不完，感兴趣的同学可以参考tensorflow中的介绍（\url{https://www.tensorflow.org/extras/candidate_sampling.pdf}）以及NLP领域的经典论文 \url{http://www.aclweb.org/anthology/P15-1001}

\subsubsection{最近邻搜索}
在candidate generation model的serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？

这个问题的答案是一个经典的工程和学术做trade-off的结果，在model serving过程中对几百万个候选集逐一跑一遍模型的时间开销显然太大了，因此在通过candidate generation model得到user 和 video的embedding之后，通过最近邻搜索的方法的效率高很多。我们甚至不用把任何model inference的过程搬上服务器，只需要把user embedding和video embedding存到redis或者内存中就好了。

\subsubsection{新视频的偏好特征}
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？

为了拟合用户对fresh content的bias，模型引入了“Example Age”这个feature，文中其实并没有精确的定义什么是example age。按照文章的说法猜测的话，会直接把sample log距离当前的时间作为example age。比如24小时前的日志，这个example age就是24。在做模型serving的时候，不管使用那个video，会直接把这个feature设成0。大家可以仔细想一下这个做法的细节和动机，非常有意思。

当然我最初的理解是训练中会把Days since Upload作为这个example age，比如虽然是24小时前的log，但是这个video已经上传了90小时了，那这个feature value就是90。那么在做inference的时候，这个feature就不会是0，而是当前时间每个video的上传时间了。

我不能100\%确定文章中描述的是那种做法，大概率是第一种。还请大家踊跃讨论。

文章也验证了，example age这个feature能够很好的把视频的freshness的程度对popularity的影响引入模型中。

\begin{framed}
\textbf{Example Age}特征\cite{Recommender_System_With_Deep_Learning_Youtube}：

我们知道，每一秒中，YouTube都有大量视频被上传，推荐这些最新视频对于YouTube来说是极其重要的。同时，通过观察历史数据发现，用户更倾向于推荐那些尽管相关度不高但是是最新（fresh）的视频。看论文的图片，我们可能认为该特征表示视频被上传之后距现在的时间。但文章其实没有定义这个特征是如何获取到的，应该是训练时间-Sample Log的产生时间。而在线上服务阶段，该特征被赋予0值甚至是一个比较小的负数。这样的做法类似于在广告排序中消除position bias。

假设这样一个视频十天前发布的，许多用户在当前观看了该视频，那么在当天会产生许多Sample Log，而在后面的九天里，观看记录不多，Sample Log也很少。如果我们没有加入Example Age这个特征的话，无论何时训练模型，这个视频对应的分类概率都是差不多的，但是如果我们加入这个特征，模型就会知道，如果这条记录是十天前产生的话，该视频会有很高的分类概率，如果是最近几天产生的话，分类概率应该低一些，这样可以更加逼近实际的数据。
\end{framed}

\subsubsection{采样训练样本}
在对训练集的预处理过程中，YouTube没有采用原始的用户日志，而是对每个用户提取等数量的训练样本，这是为什么？

原文的解答是这样的：Another key insight that improved live metrics was to generate a fixed number of training examples per user, effectively weighting our users equally in the loss function. This prevented a small cohort of highly active users from dominating the loss. 理由很简单，这是为了减少高度活跃用户对于loss的过度影响。

\begin{framed}
\textbf{样本和上下文选择}

正样本是用户所有完整观看过的视频，其余可以视作负样本。

训练样本是从Youtube所有的用户观看记录里产生的，而并非只是通过推荐系统产生的。同时，\textbf{针对每一个用户的观看记录，都生成了固定数量的训练样本，这样，每个用户在损失函数中的地位都是相等的，防止一小部分超级活跃用户主导损失函数} 。
\end{framed}

\subsubsection{历史的时序特征}
YouTube为什么不采取类似RNN的Sequence model，而是完全摒弃了用户观看历史的时序特征，把用户最近的浏览历史等同看待，这不会损失有效信息吗？

这个原因应该是YouTube工程师的“经验之谈”，如果过多考虑时序的影响，用户的推荐结果将过多受最近观看或搜索的一个视频的影响。YouTube给出一个例子，如果用户刚搜索过“tayer swift”，你就把用户主页的推荐结果大部分变成tayer swift有关的视频，这其实是非常差的体验。为了综合考虑之前多次搜索和观看的信息，YouTube丢掉了时序信息，讲用户近期的历史纪录等同看待。

但RNN到底适不适合next watch这一场景，其实还有待商榷，目前youtube已经上线了以RNN为基础的推荐模型， 参考论文如下： \url{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46488.pdf}

\subsubsection{生成测试集}
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？

这个问题比较好回答，只留最后一次观看行为做测试集主要是为了避免引入future information，产生与事实不符的数据穿越。

\subsubsection{优化目标}
在确定优化目标的时候，YouTube为什么不采用经典的CTR，或者播放率（Play Rate），而是采用了每次曝光预期播放时间（expected watch time per impression）作为优化目标？

这个问题从模型角度出发，是因为 watch time更能反应用户的真实兴趣，从商业模型角度出发，因为watch time越长，YouTube获得的广告收益越多。而且增加用户的watch time也更符合一个视频网站的长期利益和用户粘性。

这个问题看似很小，实则非常重要，objective的设定应该是一个算法模型的根本性问题，而且是算法模型部门跟其他部门接口性的工作，从这个角度说，YouTube的推荐模型符合其根本的商业模型，非常好的经验。

\subsubsection{video embedding}
在进行video embedding的时候，为什么要直接把大量长尾的video直接用0向量代替？

这又是一次工程和算法的trade-off，把大量长尾的video截断掉，主要还是为了节省online serving中宝贵的内存资源。当然从模型角度讲，低频video的embedding的准确性不佳是另一个“截断掉也不那么可惜”的理由。

当然，之前很多同学在评论中也提到简单用0向量代替并不是一个非常好的选择，那么有什么其他方法，大家可以思考一下。

\subsubsection{特征的特殊处理}
针对某些特征，比如\#previous impressions，为什么要进行开方和平方处理后，当作三个特征输入模型？

这是很简单有效的工程经验，引入了特征的非线性。从YouTube这篇文章的效果反馈来看，提升了其模型的离线准确度。

\subsubsection{输出层}
为什么ranking model不采用经典的logistic regression当作输出层，而是采用了weighted logistic regression？

因为在第7问中，我们已经知道模型采用了expected watch time per impression作为优化目标，所以如果简单使用LR就无法引入正样本的watch time信息。因此采用weighted LR，将watch time作为正样本的weight，在线上serving中使用e(Wx+b)做预测可以直接得到expected watch time的近似，完美。

\subsection{详细解读输出层采用 weighted logistic regression}
\subsubsection{传统套路}
对于传统的深度学习架构，输出层往往采用LR或者Softmax，在线上预测过程中，也是原封不动的照搬LR或者softmax的经典形式来计算点击率（广义地说，应该是正样本概率）。

而YouTube这一模型的神奇之处在于，\textbf{输出层没有使用LR，而是采用了Weighted LR，模型serving没有采用sigmoid函数的形式，而是使用了
$e^{Wx+b}$这一指数形式}。按照原文说法，这样做预测的就是用户观看时长？？

搞清楚这件事情并不是一件容易的事情，我们要从逻辑回归的本质意义上开始。

\subsubsection{逻辑回归溯源和 Odds}
几乎所有算法工程师的第一堂课就是逻辑回归，也肯定知道逻辑回归的数学形式就是一个线性回归套sigmoid函数：
$$
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}
$$

但为什么选择sigmoid函数？难道仅仅是sigmoid函数能把值域映射到0-1之间，符合概率的物理意义这么简单吗？

为解释这个问题，首先我们需要定义一个新的变量——\textbf{Odds}，中文可以叫发生比或者机会比。
$$
Odds = \frac{p}{1-p}
$$

假设一件事情发生的概率是p，那么\textbf{Odds就是一件事情发生和不发生的比值}。

如果对Odds取自然对数，再让ln(Odds)等于一个线性回归函数，那么就得到了下面的等式。
$$
logit(p) = \ln(\frac{p}{1-p}) = \theta_0 + \theta_1x_1 + \theta_2x_2
$$

其中$\ln(p/(1-p))$就是大名鼎鼎的logit函数，logistics regression又名logit regression，上面的式子就是逻辑回归的由来。我们再做进一步运算，就可以转变成我们熟悉的逻辑回归的形式：
$$
 \ln(\frac{p}{1-p}) = \theta^Tx \Rightarrow \frac{p}{1-p} = e^{\theta^Tx} \Rightarrow p = \frac{1}{1+e^{-\theta^Tx}} \Rightarrow p = sigmoid(\theta^Tx)
$$

在短视频的CTR预估\cite{Understand_Weighted_LR}，一般的，点击发生的概率就是发生点击的视频个数/总共曝光的视频个数，假设发生点击的视频个数为 $M$，总共曝光的视频个数为 $N$，则 $p$ 为：
$$
p = \frac{M}{N}
$$

可得Odds为：
$$
Odds = \frac{M/N}{(N-M)/N} = \frac{M/N}{1-M/N} 
$$

到这里大家应该已经完全明白了LR的推导过程了。

那么再对$\ln(Odds) = \theta^Tx$这个等式做一个小小的转换，两边取自然底数：
$$
\ln(Odds) = \theta^Tx \Rightarrow Odds = e^{\theta^Tx} = \text{YouTubeServingFunction}
$$

大家看到了吗，\textbf{Youtube的Serving函数$e^{Wx+b}$计算的不是别的，正是Odds}！

但我们还没有到达终点，因为Youtube要预测的明明是用户观看时长，怎么就成了Odds了？

\subsubsection{Weight LR}
这就要提到YouTube采用的独特的训练方式Weighted LR，这里的Weight，对于正样本$i$来说就是观看时长$T_i$，对于负样本来说，则指定了单位权重1。

Weighted LR的特点是，正样本权重$w$的加入会让正样本发生的几率变成原来的$w$倍，也就是说样本i的Odds变成了下面的式子：
$$
Odds(i) = \frac{w_i * M/N}{ 1 - w_i*M/N} = \frac{w_i*p}{1 - w_i*p}
$$

\begin{framed}
注意：这里 $N$ 的物理含有与之前的 $N$ 已经不同了，之前代表的是总共曝光的视频个数，这里代表的是总共曝光的视频的权重和，但这并不影响后面的推导。
\end{framed}

由于在视频推荐场景中，用户打开一个视频的概率p往往是一个很小的值，因此上式可以继续简化：
$$
Odds(i) = \frac{w_ip}{1-w_ip} \approx w_ip = T_ip = E(T_i)
$$

而且由于YouTube采用了用户观看时长$T_i$作为权重，因此式子进一步等于$T_i*p$，这里真相就大白了，由于$p$就是用户打开视频的概率，$T_i$是观看时长，因此$T_i*p$就是用户观看某视频的期望时长！

因此，YouTube采用$e^{Wx+b}$ 这一指数形式预测的就是曝光这个视频时，用户观看这个视频的时长的期望！利用该指标排序后再进行推荐，是完全符合YouTube的推荐场景和以观看时长为优化目标的设定的。

\begin{framed}
那为什么不直接预测观看视频的期望时长呢？一个明显的好处就是，分类的问题一般都比回归问题易于求解，且预测准确率更高。另外，这里使用Weighted LR给我们的启示就是：具体算法一定要根据具体业务场景选择，深刻理解业务的通用性和特殊性，往往比技术更重要。
\end{framed}

\begin{framed}
接下来的问题就是训练了，训练Weighted LR一般来说有两种办法：

(1). 将正样本按照weight做重复sampling，然后输入模型进行训练；

(2). 在训练的梯度下降过程中，通过改变梯度的weight来得到Weighted LR。

一般采用第二种方法，原因是减少了处理的样本数，减少了读样本时间和更新梯度的次数。
\end{framed}

\textbf{再简要总结一下YouTube Ranking Model的Serving过程要点}

(1) $e^{Wx+b}$  这一指数形式计算的是Weighted LR的Odds；

(2) Weighted LR使用用户观看时长作为权重，使得对应的Odds表示的就是用户观看时长的期望；

(3) Model Serving过程中$e^{Wx+b}$计算的正是观看时长的期望。

\subsubsection{推广}
在上面推导的过程中，一个很特殊的业务场景在于在视频推荐场景中，用户打开一个视频的概率 $p$ 往往是一个很小的值，所以进行了简化。那么，当我们的业务中，概率 $p$ 并不可以忽略，那么我们将如何优化呢？

在之前的场景中，负样本的权重为1，正样本的权重为$w_i$，实际上Odds为：
$$
Odds(i) = \frac{\sum_iw_i * M_i/N}{\sum_i Neg_i/N} = \frac{\sum_{i=1}^{N_{pos}}w_i}{\sum_{i=1}^{N_{neg}}1} = \frac{\sum_{i=1}^{N_{pos}}w_i}{N_{neg}}
$$

上面是因为 $p$ 概率足够小，也就是 $N_{pos}$ 相对 $N_{neg}$ 相对很小，所以上面可以近似为：
$$
\frac{\sum_{i=1}^{N_{pos}}w_i}{N_{neg}} = \frac{\sum_{i=1}^{N_{pos}}w_i}{N_{neg} + N_{pos}} = \frac{\sum_{i=1}^{N_{pos}}w_i}{N}
$$

当我们的场景中，概率 $p$（在推荐中即ctr）达到20\%甚至更高，直接忽略近似会boost click，那么我们就要在分母项补上$N_{pos}$的值。显然，我们可以在负样本中随机采样$N_{pos}$的样本进行填充，即可达到我们的目的。

\section{阿里深度兴趣网络（DIN）}
\subsection{背景\cite{Recommender_System_With_Deep_Learning_DIN}}
深度学习在CTR预估领域已经有了广泛的应用，常见的算法比如Wide\&Deep，DeepFM等。这些方法一般的思路是：通过Embedding层，将高维离散特征转换为固定长度的连续特征，然后通过多个全联接层，最后通过一个sigmoid函数转化为0-1值，代表点击的概率。即Sparse Features -> Embedding Vector -> MLPs -> Sigmoid -> Output。

这种方法的优点在于：通过神经网络可以拟合高阶的非线性关系，同时减少了人工特征的工作量。

不过，阿里的研究者们通过观察收集到的线上数据，发现了用户行为数据中有两个很重要的特性：
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textbf{Diversity}：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的；
    \item \textbf{Local activation}: 由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录。
\end{itemize}

这两种特性是密不可分的。举个简单的例子，观察下面的表格：
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/DIN_User_Interest_Table_Example.png}
\end{figure}

\textbf{Diversity}体现在年轻的母亲的历史记录中体现的兴趣十分广泛，涵盖羊毛衫、手提袋、耳环、童装、运动装等等；而爱好游泳的人同样兴趣广泛，历史记录涉及浴装、旅游手册、踏水板、马铃薯、冰激凌、坚果等等。

\textbf{Local activation}体现在，当我们给爱好游泳的人推荐goggle(护目镜)时，跟他之前是否购买过薯片、书籍、冰激凌的关系就不大了，而跟他游泳相关的历史记录如游泳帽的关系就比较密切。

针对上面提到的用户行为中存在的两种特性，阿里将其运用于自身的推荐系统中，推出了深度兴趣网路DIN。

\subsection{用户场景}
用户场景很简单，就是在一个电商网站或APP中给用户推荐广告，当然对于阿里妈妈来说，广告也是商品，所以这篇文章的广告场景其实也是一个经典的推荐场景。

好，既然要推荐，我们当然需要利用用户的历史数据了，对于一个电商来说，历史数据当然就是点击，添加购物车，下单这些行为了。论文中给了一位用户的行为序列。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Ali_User_Behavior_Sequence.png}
    \caption{用户的行为序列}
\end{figure}

显然是一个女生的行为历史啦，从最左边的手套，鞋子到右边的杯子，睡衣。要被推荐的候选商品是一件女式大衣。我们应该如何计算这件大衣的CTR呢？

\begin{framed}
如果按照之前的做法，我们会一碗水端平的考虑所有行为记录的影响，对应到模型中就是我们会用一个average pooling层把用户交互过的所有商品的embedding vector平均一下形成这个用户的user vector，机灵一点的工程师最多加一个time decay，让最近的行为产生的影响大一些，那就是在做average pooling的时候按时间调整一下权重。

但是我们仔细想一想我们自己的购买过程，其实每个用户的兴趣都是多样的，女生喜欢买衣服包包，也喜欢化妆品，甚至还为自己男朋友挑选过球衣球鞋，那么你在买大衣的时候，真的要把给男朋友买球鞋的偏好考虑进来么？具体到本文的例子中，在预测大衣的CTR这件事情上，用户浏览过杯子，跟用户浏览过另一件大衣这两个行为的重要程度是一样的吗？

这事不用问算法工程师，你就回家问问你老妈估计答案都是一定的，肯定是浏览过另一件大衣这件事的参考价值高啊。好了，就是这件你老妈都知道的事情，让阿里妈妈的算法工程师们加上了attention机制。
\end{framed}

\subsection{模型设计\cite{Recommender_System_With_Deep_Learning_DIN}}
整个流程可以描述为：
\begin{enumerate}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item 检查用户历史行为数据；
    \item 使用matching module产生候选ads；
    \item 通过ranking module做point-wise的排序，即得到每个候选ads的点击概率，并根据概率排序得到推荐列表；
    \item 记录下用户在当前展示广告下的反应(点击与否)，作为label
\end{enumerate}

\subsection{注意力机制}
注意力机制顾名思义，就是模型在预测的时候，对用户不同行为的注意力是不一样的，“相关”的行为历史看重一些，“不相关”的历史甚至可以忽略。那么这样的思想反应到模型中也是直观的。
$$
V_u  = f(V_a) = \sum_{i=1}^Nw_i * V_i = \sum_{i=1}^Ng(V_i, V_a) * V_i
$$

其中，$V_u$是用户的 embedding 向量，$V_a$是候选广告商品的 embedding 向量，$V_i$ 是用户$u$的第$i$次行为的 embedding 向量，因为这里用户的行为就是浏览商品或店铺，所以行为的embedding的向量就是那次浏览的商品或店铺的embedding向量。

因为加入了注意力机制，$V_u$ 从过去 $V_i$ 的\textbf{加和}变成了$V_u$ 的\textbf{加权和}， $V_i$ 的权重 $w_i$ 就由 $V_i$ 与 $V_a$ 的关系决定，也就是上式中的 $g(V_i, V_a)$。

\begin{framed}
例如，针对不同的广告，用户历史行为与该广告的权重是不同的。假设用户有 A、B、C 三个历史行为，对于广告 D，那么 A、B、C 的权重可能是 0.8、0.1、0.1；对于广告 E，那么 A、B、C 的权重可能是0.3、0.6、0.1。这里的权重，就是Attention机制下图中的Activation Unit所需要学习的。

为什么要引入这一个机制呢？难道仅仅是通过观察历史数据拍脑袋决定的么？当然不是，如果不用 Local activation 的话，将会出现下面的情况：假设用户的兴趣的 Embedding 是 $V_u$，候选广告的 Embedding是 $V_a$，用户兴趣和候选的广告的相关性可以写作
$$
F(U,A) = V_a * V_u
$$。

如果没有Local activation机制的话，那么同一个用户对于不同的广告，$V_u$ 都是相同的。举例来说，如果有两个广告 A 和 B，用户兴趣和A，B的相似性都很高，那么在$V_a$和$V_b$连线上的广告都会有很高的相似性。这样的限制使得模型非常难学习到有效的用户和广告的embedidng表示。
\end{framed}

那么 $g(V_i, V_a)$ 这个函数到底采用什么比较好呢？看完下面的架构图自然就清楚了。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/Ali_DIN_Structure.jpg}
    \caption{用户的行为序列}
\end{figure}

左边是标准的模型，其中 element-wise 的意思其实就是元素级别的加减。注意，一个元素和一个向量相乘，也可以看作element-wise的，因为这个元素会广播成和向量一样的长度。

可以看到，Base Model首先吧one-hot或multi-hot特征转换为特定长度的embedding，作为模型的输入，然后经过一个DNN的part，得到最终的预估值。特别地，针对multi-hot的特征，做了一次element-wise+的操作，这里其实就是sum-pooling，这样，不管特征中有多少个非0值，经过转换之后的长度都是一样的。

相比左边这个标准的深度推荐网络（Base model），DIN在生成用户embedding vector的时候加入了一个activation unit层，这一层产生了每个用户行为$V_i$ 的权重，下面我们仔细看一下这个权重是怎么生成的，也就是 $g(V_i, V_a)$ 是如何定义的。

传统的Attention机制中，给定两个item embedding，比如 $u$ 和 $v$，通常是直接做点积 $uv$ 或者 $uWv$，其中 $W$ 是一个 $|u|\times |v|$ 的权重矩阵，但这篇paper中阿里显然做了更进一步的改进，着重看上图右上角的activation unit，\textbf{首先是把 $u$ 和 $v$ 以及 $u v$ 的element wise差值向量合并起来作为输入，然后喂给全连接层，最后得出权重}，这样的方法显然损失的信息更少。但如果你自己想方便的引入attention机制的话，不妨先从点积的方法做起尝试一下，因为这样连训练都不用训练。

再稍微留意一下这个架构图中的红线，你会发现每个ad会有 good\_id, shop\_id 两层属性，shop\_id只跟用户历史中的shop\_id序列发生作用，good\_id只跟用户的good\_id序列发生作用，这样做的原因也是显而易见的。

\subsection{模型的一些细节}
\subsubsection{评价指标GAUC}
模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：
$$
GAUC = \frac{\sum_{i=1}^nw_i*AUC_i}{\sum_{i=1}^nw_i} = \frac{\sum_{i=1}^n impression_i * AUC_i}{\sum_{i=1}^nimpression_i}
$$

我们首先要肯定的是，\textbf{AUC是要分用户看的，我们的模型的预测结果，只要能够保证对每个用户来说，他想要的结果排在前面就好了}。

假设有两个用户A和B，每个用户都有10个商品，10个商品中有5个是正样本，我们分别用TA，TB，FA，FB来表示两个用户的正样本和负样本。也就是说，20个商品中有10个是正样本。假设模型预测的结果大小排序依次为TA，FA，TB，FB。如果把两个用户的结果混起来看，AUC并不是很高，因为有5个正样本排在了后面，但是分开看的话，每个用户的正样本都排在了负样本之前，AUC应该是1。显然，分开看更容易体现模型的效果，这样消除了用户本身的差异。

但是上文中所说的差异是在用户点击数即样本数相同的情况下说的。还有一种差异是用户的展示次数或者点击数，如果一个用户有1个正样本，10个负样本，另一个用户有5个正样本，50个负样本，这种差异同样需要消除。那么GAUC的计算，不仅将每个用户的AUC分开计算，同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理。进一步消除了用户偏差对模型的影响。通过实验证明，GAUC确实是一个更加合理的评价指标。

\subsubsection{自适应正则 Adaptive Regularization}
CTR中输入稀疏而且维度高，通常的做法是加入L1、L2、Dropout等防止过拟合。但是论文中尝试后效果都不是很好。\textbf{用户数据符合长尾定律long-tail law，也就是说很多的feature id只出现了几次，而一小部分feature id出现很多次。这在训练过程中增加了很多噪声，并且加重了过拟合}。

对于这个问题一个简单的处理办法就是：直接去掉出现次数比较少的feature id。但是这样就人为的丢掉了一些信息，导致模型更加容易过拟合，同时阈值的设定作为一个新的超参数，也是需要大量的实验来选择的。

因此，阿里提出了自适应正则的做法，即：
\begin{enumerate}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item 针对feature id出现的频率，来自适应的调整他们正则化的强度；
    \item 对于出现频率高的，给与较小的正则化强度；
    \item 对于出现频率低的，给予较大的正则化强度。
\end{enumerate}

计算公式略。

\subsection{论文作者的评论}
其实最初的时候我们不是按照借鉴attention的套路来的，当时想法就是希望做个反向激活的网络结构，能自适应地挑跟candidate相关的用户行为来做用户兴趣的表征，后来写论文的时候仔细survey发现这个是attention的解法。再透露个细节，其实在DL模型之前，我们就研制了一种可以认为是hard模式的attention结构：那是特征组合的解法，我们用ad的shop属性去hit用户历史行为过的shop list，如果命中了那么就说明历史用户有过直接行为，用行为id和频次来表示这个组合特征；如果没有命中特征就是空的，那显然attention结构就是一种自然的soft扩展，不过适用范围更强了

%\printbibliography
\bibliography{../ref}
\bibliographystyle{IEEEtran}
\end{document}